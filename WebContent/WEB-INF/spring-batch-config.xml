<?xml version="1.0" encoding="UTF-8"?>
<beans xmlns="http://www.springframework.org/schema/beans"
	xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
	xmlns:util="http://www.springframework.org/schema/beans"
	xmlns:batch="http://www.springframework.org/schema/batch"
	xmlns:task="http://www.springframework.org/schema/task"
	xmlns:context="http://www.springframework.org/schema/context"
	xmlns:p="http://www.springframework.org/schema/p"
	xsi:schemaLocation="http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context-4.1.xsd http://www.springframework.org/schema/util http://www.springframework.org/schema/util/spring-util-4.1.xsd http://www.springframework.org/schema/batch http://www.springframework.org/schema/batch/spring-batch-3.0.xsd http://www.springframework.org/schema/task http://www.springframework.org/schema/task/spring-task-4.1.xsd">

    <context:annotation-config />
    <context:component-scan base-package="jobs" />

	<task:annotation-driven executor="batchTaskExecutor" scheduler="scheduling" exception-handler="myAsyncUncaughtExceptionHandler" />

	<bean id="myAsyncUncaughtExceptionHandler" class="exceptions.MyAsyncUncaughtExceptionHandler" />

	<!-- By default when specifying @Async on a method, the executor that will be used is the one supplied to the 'annotation-driven' -->
	<!-- the value attribute of the @Async annotation can be used when needing to indicate that an executor other than the default should be used -->
	<!-- set up a default executor -->
	<!-- <task:executor id="batchTaskExecutor" pool-size="7" queue-capacity="10" rejection-policy="CALLER_RUNS" keep-alive="3600" /> -->
	<task:executor id="batchTaskExecutor" pool-size="03" queue-capacity="05" rejection-policy="CALLER_RUNS" keep-alive="3600" />
	<!-- <task:executor id="executorWithPoolSizeRange" pool-size="5-25" queue-capacity="100" rejection-policy="DISCARD" keep-alive="3600" /> -->
	<task:scheduler id="scheduling" pool-size="03" />

	<task:scheduled-tasks scheduler="scheduling">
		<!-- new CronTrigger("* 15 9-17 * * MON-FRI") -->
		<!-- java.lang.IllegalArgumentException: Cron expression must consist of 6 fields (found 7 in "0 00 0 1 APR,MAY,JUN ? 2018") -->

		<!-- <task:scheduled ref="jobs" method="run_database_maintenance" cron="0 30 01 ? * *"  /> -->
  		<!-- <task:scheduled ref="jobs" method="run_pam" cron="0 41 11 ? * *" /> -->
  		<!-- <task:scheduled ref="jobs" method="run_pam" cron="30 01 00 ? * *" /> -->
  		<task:scheduled ref="jobs" method="run_pam" cron="30 01/12 * ? * *" />

  		<!-- <task:scheduled ref="jobs" method="renew_crbt" cron="0 30 02 ? * *" /> -->
  		<task:scheduled ref="jobs" method="renew_crbt" cron="0 5/12 * 5/1 * ?" />

		<!-- <task:scheduled ref="jobs" method="setHappyBirthDayBonus" cron="0 30 07 ? * *" /> -->
		<task:scheduled ref="jobs" method="setHappyBirthDayBonus" cron="0 03/12 * 5/1 * ?" />

  		<!-- <task:scheduled ref="jobs" method="clear_ussd" fixed-delay="900000" /> -->
  		<task:scheduled ref="jobs" method="clear_ussd" fixed-delay="720000" />
	</task:scheduled-tasks>

    <bean id="transactionManager" class="org.springframework.batch.support.transaction.ResourcelessTransactionManager" />
    <bean id="jobRepository" class="org.springframework.batch.core.repository.support.MapJobRepositoryFactoryBean" p:transactionManager-ref="transactionManager" />
    <bean id="jobLauncher" class="org.springframework.batch.core.launch.support.SimpleJobLauncher" p:jobRepository-ref="jobRepository" p:taskExecutor-ref="batchTaskExecutor" />
 	<!-- <bean id="jobLauncher" class="org.springframework.batch.core.launch.support.SimpleJobLauncher" p:jobRepository-ref="jobRepository">
	  	<property name="taskExecutor">
	    	<bean class="org.springframework.core.task.SimpleAsyncTaskExecutor" />
	  	</property>
  	</bean> -->

	<batch:job id="cleanExpiredUssdRequestJob">
	  <batch:step id="cleanUssdRequest">
	  	<!-- <batch:tasklet ref="cleanExpiredUssdRequestTasklet" transaction-manager="transactionManager" /> -->
	  	<batch:tasklet transaction-manager="transactionManager">
          <bean class="jobs.CleanExpiredUssdRequestTasklet">
     		<property name="productProperties" ref="productProperties" />
     		<property name="dao" ref="dao" />
     	  </bean>
	  	</batch:tasklet>
	  </batch:step>
	</batch:job>

	<batch:job id="runningPAMJob">
	  <batch:listeners>
      	<!-- <batch:listener>
          <bean class="jobs.listeners.JobPhaseEventListener" />
        </batch:listener> -->
        <batch:listener ref="jobPhaseEventListener" />
  	  </batch:listeners>

	  <batch:step id="runningPAM">
	  	<!-- throttle-limit, this attribute configures the level of thread concurrency and has a default value of 4 -->
	  	<!-- Ensure the core pool size is larger than this limit. -->
	  	<!-- <batch:tasklet task-executor="MultithreadedStepsTaskExecutor" throttle-limit="5"> -->
	  	<batch:tasklet task-executor="MultithreadedStepsTaskExecutor" throttle-limit="4">
	  	<!-- <batch:tasklet> -->
	      <!-- <batch:chunk reader="subscribers" processor="runningPAMProcessor" writer="runningPAMWriter" commit-interval="100" /> -->
	      <!-- <batch:chunk reader="subscribers" processor="runningPAMProcessor" writer="runningPAMWriter" commit-interval="50" /> -->
	      <!-- <batch:chunk reader="subscribers" processor="runningPAMProcessor" writer="runningPAMWriter" commit-interval="100" skip-limit="10" retry-limit="3"> -->
	      <!-- <batch:chunk reader="subscribers" commit-interval="100" skip-limit="25" retry-limit="3"> -->
	      <batch:chunk reader="runningPAMSubscribers" commit-interval="20" skip-limit="20" retry-limit="3">

	        <batch:processor>
	    	  <!-- CompositeItemProcessor -->
	    	  <!-- You break up a step into three phases (reading, processing, and writing) to divide responsibilities 
					between components.  However, the business logic that needs to be applied to a given item may not 
					make sense to couple into a single ItemProcessor.  Spring Batch allows you to maintain that same 
					division of responsibilities within your business logic by chaining ItemProcessors within a step.  In this 
					section, you will look at how to chain ItemProcessors within a single step using Spring Batch’s 
					CompositeItemProcessor.

					The org.springframework.batch.item.support.CompositeItemProcessor is an implementation of the 
					ItemProcessor interface that delegates processing to each of a list of ItemProcessor implementations in 
					order.  As each processor returns its result, that result is passed onto the next processor until they all 
					have been called.  This pattern occurs regardless of the types returned so if the first ItemProcessor takes 
					a String as input it can return a Product object as output as long as the next ItemProcessor takes a 
					Product as input.  At the end, the result is passed to the ItemWriter configured for the step.  It is 
					important to note that just like any other ItemProcessor, if any of the processors this one delegates to 
					returns null, the item will not be process further.  Figure 8-2 shows how the processing within the 
					CompositeItemProcessor occurs. 

					<bean id="completeItemProcessor" class="org.springframework.batch.item.support.CompositeItemProcessor"> 
        				<property name="delegates"> 
            				<util:list> 
                				<ref bean="customerIdItemProcessor"/> 
                				<ref bean="accountExecutiveItemProcessor"/> 
            				</util:list> 
        				</property> 
    				</bean>
    				
    				we can combine in this manner the run pam and notification sms steps. But because of synchronization (be sure DA is provided and already available after pam running before reading DA) and faster granting bonus to subscribers (priority is subscriber get night advantages bonus as soon and fast as possible and not send sms notification) reasons, we separate processing into two phases or steps
			  -->
	          <bean class="jobs.RunningPAMProcessor" scope="step">
      			<property name="waitingForResponse" value="false" />
      			<property name="productProperties" ref="productProperties" />
      		  </bean>
	        </batch:processor>

	        <batch:writer>
	          <bean class="jobs.RunningPAMWriter">
	          	<property name="waitingForResponse" value="false" />
	          	<property name="dao" ref="dao" />
	          </bean>
	        </batch:writer>

			<!-- The Step allows a limit for the number of times an exception can be skipped, and a list of exceptions that are 'skippable'. -->
         	<batch:skippable-exception-classes>
            	<batch:include class="exceptions.AirAvailabilityException" />

            	<!-- <batch:include class="org.springframework.batch.item.file.FlatFileParseException" />
            	<batch:exclude class="java.lang.Exception" />
            	<batch:exclude class="java.io.FileNotFoundException" /> -->
         	</batch:skippable-exception-classes>

			<!-- The Step allows a limit for the number of times an individual item can be retried, and a list of exceptions that are 'retryable'. -->
	        <batch:retryable-exception-classes>
	          	<batch:include class="org.springframework.dao.DeadlockLoserDataAccessException"/>
	        </batch:retryable-exception-classes>
	      </batch:chunk>

		  <!-- the Step can be configured with a list of exceptions that should not cause rollback. -->
	      <batch:no-rollback-exception-classes>
	         <batch:include class="java.lang.Throwable" />
	      </batch:no-rollback-exception-classes>

		  <!-- Intercepting Step Execution -->
		  <!-- Just as with the Job, there are many events during the execution of a Step where a user may need to perform some functionality.
		  This can be accomplished with one of many Step scoped listeners. -->
    	  <batch:listeners>
    	  	<batch:listener ref="jobRunListener" />

      		<batch:listener>
      			<bean class="jobs.listeners.StagingRunningPAMStepListener" scope="step">
      				<property name="dao" ref="dao" />
      				<property name="productProperties" ref="productProperties" />
      			</bean>
      		</batch:listener>

			<!-- Configuring the CustomerItemListener. It is useful when Logging Invalid Records as an example -->
    	    <!-- Logging Invalid Records -->
    	    <!-- While skipping problematic records is a useful tool, by itself it can raise an issue. In some scenarios, the 
			ability to skip a record is okay. Say you are mining data and come across something you can’t resolve; it’s 
			probably okay to skip it. However, when you get into situations where money is involved, say when 
			processing transactions, just skipping a record probably will not be a robust enough solution. In cases 
			like these, it is helpful to be able to log the record that was the cause of the error. In this section, you will 
			look at using an ItemListener to record records that were invalid.  

			 <batch:skippable-exception-classes>
            	<batch:exclude class="java.lang.Exception" />
         	</batch:skippable-exception-classes>

			If you use the fixed length record job as an example and execute it with a file that contains an input 
			record longer than 63 characters (FlatFileItemReader example with a threshold FixedLengthTokenizer), an exception (FlatFileParseException) will be thrown. However, since you have configured your 
			job to skip all exceptions that extend Exception, the exception will not affect your job’s results, yet your 
			subscriberItemLogger will be called and log the item as required. -->
      		<batch:listener>
      			<!-- <bean id="subscriberItemLogger" class="jobs.listeners.SubscriberItemListener" scope="step"> -->
      			<bean id="subscriberItemLogger" class="jobs.listeners.SubscriberItemListener">
      				<property name="dao" ref="dao" />
      				<property name="productProperties" ref="productProperties" />
      			</bean>
      		</batch:listener>

			<!-- Configuring the CustomerSkipListener -->
    	    <!-- it is less global than CustomerItemListener and treats only item skipped. It is useful when Logging Invalid Records as an example -->
			<batch:listener ref="itemProcessingSkipListener" />

      		<batch:listener>
      			<bean class="jobs.listeners.StagingRunningPAMChunkUpdater" scope="step" />
      		</batch:listener>
    	  </batch:listeners>
	    </batch:tasklet>

		<!-- Declares job should end at this point, without the possibility of restart. BatchStatus will be COMPLETED. ExitStatus is configurable. -->
		<!-- The 'end' element instructs a Job to stop with a BatchStatus of COMPLETED. A Job that has finished with status COMPLETED cannot be restarted (the framework will throw a JobInstanceAlreadyCompleteException) -->
		<!-- The 'end' element also allows for an optional 'exit-code' attribute that can be used to customize the ExitStatus of the Job. If no 'exit-code' attribute is given, then the ExitStatus will be "COMPLETED" by default, to match the BatchStatus. -->
    	<!-- <batch:end on="FAILED" /> -->
    	<!-- <batch:end on="COMPLETED WITH SKIPS" to="errorPrint1" /> -->

		<!-- Declares job should fail at this point. BatchStatus will be FAILED. ExitStatus is configurable. -->
		<!-- The 'fail' element instructs a Job to stop with a BatchStatus of FAILED. Unlike the 'end' element, the 'fail' element will not prevent the Job from being restarted.
		The 'fail' element also allows for an optional 'exit-code' attribute that can be used to customize the ExitStatus of the Job. If no 'exit-code' attribute is given, then the ExitStatus will be "FAILED" by default, to match the BatchStatus. -->
		<!-- <batch:fail on="FAILED" exit-code="EARLY TERMINATION" /> -->

		<!-- Declares job should be stop at this point and provides pointer where execution should continue when the job is restarted. -->
		<!-- The 'stop' element instructs a Job to stop with a BatchStatus of STOPPED. Stopping a Job can provide a temporary break in processing so that the operator can take some action before restarting the Job. -->
		<!-- The 'stop' element requires a 'restart' attribute that specifies the step where execution should pick up when the Job is restarted. -->
		<!-- <batch:stop on="COMPLETED" restart="step2" /> -->

		<!-- If transition elements are used, then all of the behavior for the Step's transitions must be defined explicitly. While there is no limit to the number of transition elements on a Step, if the Step's execution results in an ExitStatus that is not covered by an element, then the framework will throw an exception and the Job will fail. The framework will automatically order transitions from most specific to least specific. -->
		<!-- Defines a transition from this step to the next one depending on the value of the exit status. ExitStatus represents the status of a Step after it finishes execution. More specifically, the 'next' element above references the exit code of the ExitStatus -->
    	<!-- <batch:next on="COMPLETED WITH SKIPS" to="errorPrint1" /> -->
    	<!-- <batch:next on="*" to="step2" /> -->
    	<batch:next on="COMPLETED" to="nightAdvantagesNotificationThroughSms" />
	  </batch:step>

	  <batch:step id="nightAdvantagesNotificationThroughSms">
	  	<batch:tasklet task-executor="MultithreadedStepsTaskExecutor" throttle-limit="4">
	      <!-- <batch:chunk commit-interval="100" skip-limit="20" retry-limit="3"> -->
	      <batch:chunk commit-interval="50" skip-limit="20" retry-limit="3">
	        <batch:reader>
				<bean class="jobs.SynchronizingNightAdvantagesSubscriberReader">
				  <property name="delegate">
			  		<!-- reader must be evaluated for every scheduled execution of the given job. -->
			  		<!-- If you set the scope of the bean to step, then a new bean will be created every time the step is executed. -->
					<!-- <bean class="org.springframework.batch.item.database.JdbcCursorItemReader" scope="step"> -->
					<bean class="jobs.NightAdvantagesSubscriberReader" scope="step">
					  	<property name="dataSource" ref="db_connection" />
					  	<!-- <property name="sql">
					  		<value>
					    		SELECT ID,MSISDN,FLAG,CRBT,LAST_UPDATE_TIME,CRBT_NEXT_RENEWAL_DATE,LOCKED FROM MTN_KIF_MSISDN_EBA WHERE ((FLAG = 1) AND (LOCKED = 0))
					    		SELECT ID,MSISDN,FLAG,CRBT,LAST_UPDATE_TIME,CRBT_NEXT_RENEWAL_DATE,LOCKED FROM MTN_KIF_MSISDN_EBA WHERE FLAG = 1
					  		</value>
					  	</property> -->
					  	<property name="saveState" value="false" />
					  	<property name="rowMapper">
					    	<bean class="dao.mapping.PAMRunningReportingRowMapper" />
					  	</property>
					</bean>
				  </property>
				</bean>
	        </batch:reader>

	        <batch:processor>
	          <bean class="jobs.NightAdvantagesNotificationProcessor">
	          	<property name="dao" ref="dao" />
	          	<property name="productProperties" ref="productProperties" />
	          	<property name="waitingForResponse" value="false" />
	          </bean>
	        </batch:processor>

	        <batch:writer>
	          <bean class="jobs.NightAdvantagesNotificationWriter">
	          	<property name="i18n" ref="messageSource" />
	          	<property name="productProperties" ref="productProperties" />
	          </bean>
	        </batch:writer>

			<!-- The Step allows a limit for the number of times an exception can be skipped, and a list of exceptions that are 'skippable'. -->
         	<batch:skippable-exception-classes>
            	<batch:include class="exceptions.AirAvailabilityException" />

            	<!-- <batch:include class="org.springframework.batch.item.file.FlatFileParseException" />
            	<batch:exclude class="java.lang.Exception" />
            	<batch:exclude class="java.io.FileNotFoundException" /> -->
         	</batch:skippable-exception-classes>

			<!-- The Step allows a limit for the number of times an individual item can be retried, and a list of exceptions that are 'retryable'. -->
	        <batch:retryable-exception-classes>
	          	<batch:include class="org.springframework.dao.DeadlockLoserDataAccessException" />
	        </batch:retryable-exception-classes>
	      </batch:chunk>

		  <!-- the Step can be configured with a list of exceptions that should not cause rollback. -->
	      <batch:no-rollback-exception-classes>
	         <batch:include class="java.lang.Throwable" />
	      </batch:no-rollback-exception-classes>

		  <!-- Intercepting Step Execution -->
		  <!-- Just as with the Job, there are many events during the execution of a Step where a user may need to perform some functionality.
		  This can be accomplished with one of many Step scoped listeners. -->
    	  <batch:listeners>
    	  	<batch:listener ref="jobRunListener" />
			<batch:listener ref="itemProcessingSkipListener" />
    	  </batch:listeners>
	    </batch:tasklet>
	  </batch:step>
	</batch:job>

	<batch:job id="crbtRenewalJob">
	  <batch:listeners>
      	<!-- <batch:listener>
          <bean class="jobs.listeners.JobPhaseEventListener" />
        </batch:listener> -->
        <batch:listener ref="jobPhaseEventListener" />
  	  </batch:listeners>

	  <batch:step id="crbtRenewal">
	  	<!-- <batch:tasklet ref="crbtRenewalTasklet" transaction-manager="transactionManager" /> -->
	  	<batch:tasklet transaction-manager="transactionManager">
          <bean class="jobs.CRBTRenewalTasklet">
          	<property name="i18n" ref="messageSource" />
     		<property name="productProperties" ref="productProperties" />
     		<property name="dao" ref="dao" />
     	  </bean>
		  <!-- Intercepting Step Execution -->
		  <!-- Just as with the Job, there are many events during the execution of a Step where a user may need to perform some functionality.
		  This can be accomplished with one of many Step scoped listeners. -->
    	  <batch:listeners>
    	  	<batch:listener ref="jobRunListener" />
    	  </batch:listeners>
	    </batch:tasklet>
	  </batch:step>
	</batch:job>

	<!-- To disable restart, set the attribute restartable to false on the job element:
	Remember that jobs are restartable by default. If you’re worried that you’ll forget that, set the restartable flag explicitly on all your jobs. -->
	<batch:job id="happyBirthDayBonusJob">
	  <batch:listeners>
      	<!-- <batch:listener>
          <bean class="jobs.listeners.JobPhaseEventListener" />
        </batch:listener> -->
        <batch:listener ref="jobPhaseEventListener" />
  	  </batch:listeners>

	  <!-- A single step cannot have both a "next" attribute and a transition element. -->
	  <!-- <batch:step id="stepA" next="stepB" /> -->
	  <batch:step id="happyBirthDayBonus">
	  	<!-- <batch:tasklet task-executor="MultithreadedStepsTaskExecutor" throttle-limit="5"> -->
	  	<batch:tasklet task-executor="MultithreadedStepsTaskExecutor" throttle-limit="4">
	      <!-- <batch:chunk reader="allMTNKIFSubscribersWithNoHappyBirthDayBonusReader" processor="happyBirthdayBonusProcessor" writer="happyBirthdayBonusWriter" commit-interval="10" skip-limit="20" retry-limit="3"> -->
	      <batch:chunk reader="allMTNKIFSubscribersWithNoHappyBirthDayBonusReader" commit-interval="10" skip-limit="20" retry-limit="3">

	        <batch:processor>
	    	  <!-- CompositeItemProcessor -->
	    	  <!-- You break up a step into three phases (reading, processing, and writing) to divide responsibilities 
					between components.  However, the business logic that needs to be applied to a given item may not 
					make sense to couple into a single ItemProcessor.  Spring Batch allows you to maintain that same 
					division of responsibilities within your business logic by chaining ItemProcessors within a step.  In this 
					section, you will look at how to chain ItemProcessors within a single step using Spring Batch’s 
					CompositeItemProcessor.

					The org.springframework.batch.item.support.CompositeItemProcessor is an implementation of the 
					ItemProcessor interface that delegates processing to each of a list of ItemProcessor implementations in 
					order.  As each processor returns its result, that result is passed onto the next processor until they all 
					have been called.  This pattern occurs regardless of the types returned so if the first ItemProcessor takes 
					a String as input it can return a Product object as output as long as the next ItemProcessor takes a 
					Product as input.  At the end, the result is passed to the ItemWriter configured for the step.  It is 
					important to note that just like any other ItemProcessor, if any of the processors this one delegates to 
					returns null, the item will not be process further.  Figure 8-2 shows how the processing within the 
					CompositeItemProcessor occurs. 
			  -->
			  <bean id="completeItemProcessor" class="org.springframework.batch.item.support.CompositeItemProcessor">
      			<property name="delegates"> 
          			<util:list> 
              			<!-- <ref bean="customerIdItemProcessor"/> 
              			<ref bean="accountExecutiveItemProcessor"/> -->
              			
              			<!-- Validating items -->
              			<!-- Because validation is business logic, the standard location to enforce validation rules is in the item-processing phase of a chunk-oriented step.
              			A common practice in Spring Batch is for an item processor to perform validation checks on read items and decide whether to send the items to the item writer.
              			As an example, let’s see how to validate the price of imported products and check that prices aren’t negative numbers (products with  a  negative  price  shouldn’t  reach  the  database—you  don’t  want  to  credit  your customers!).
              			Should  you  consider  an  item  that  fails  the  validation  check  filtered  or skipped? Skipping is semantically closer to a validation failure, but this remains questionable, and the business requirements usually lead to the correct answer.
              			
              			A validation failure should lead to a skipped or filtered item, but what you care about is that the item writer doesn’t receive the item in question.
              			Remember that the corresponding step-execution metadata stored in the job repository is distinct (skip and filter count), and this distinction can be relevant for some use cases.
              			If you want to enforce validation rules in your item processor, use the following semantics for validation failure in the item processor’s process method:
              				If validation means skip, throw a runtime exception
              				If validation means filter, return null
              			
              			What kind of validation can an item processor perform? You can do almost anything: state validation of the read object, consistency check with other data, and so forth.
              			In the import products job, for example, you can check that the price of products from the flat file is positive.
              			A well-formatted negative price would pass the reading phase (no parsing exception), but you shouldn’t write the product to the database.
              			The job of the item processor is to enforce this check and discard invalid products.
              			You can implement an item processor corresponding to this example and follow the semantics outlined here, but Spring Batch already provides a class called ValidatingItemProcessor to handle this task.
              			
              			The Spring Batch class ValidatingItemProcessor has two interesting characteristics:
              				It  delegates  validation  to  an  implementation  of  the  Spring  Batch  Validator interface.
              				It has a filter flag that can be set to false to throw an exception (skip) or true  to  return  null  (filter)  if  the  validation  fails.  The  default  value  is  false (skip).
              			By using the ValidatingItemProcessor class, you can embed your validation rules in dedicated Validator implementations (which you can reuse) and choose your validation semantics by setting the filter property.
              			When you decide to use the ValidatingItemProcessor class, you can either code your validation logic in Validator implementations or create a Validator bridge to a full-blown validation framework. -->
              			<bean class="org.springframework.batch.item.validator.ValidatingItemProcessor">
              				<!-- If filter property is set to false (this is the default value), the item processor rethrows any ValidationException thrown by its validator to enforce skipping. This implies the configuration of a skip strategy if you don’t want to fail the whole job execution in case of a validation failure. -->
              				<!-- <property name="filter" value="false" /> -->

              				<!-- If filter property is set to true, item becomes null after validation phase and is filtered (not handled according the business rule) -->
              				<!-- If you were only to filter products that have a negative price, you would set the filter property of the ValidatingItemProcessor to true and wouldn’t need any skip configuration. -->
              				<property name="filter" value="true" />

              				<property name="validator">
		             			<bean class="jobs.HappyBirthDayBonusSubscriberValidator">
		             				<constructor-arg index="0" type="product.ProductProperties" ref="productProperties" />
		             				<!-- <constructor-arg index="0" type="product.ProductPropertiesBasedOnPropertiesFactoryBean" ref="productProperties" /> -->
		             				<!-- <constructor-arg index="0" type="product.ProductPropertiesBasedOnPropertyPlaceholderConfigurer" ref="productProperties" /> -->
		             				<constructor-arg index="1" type="dao.DAO" ref="dao" />

		          					<!-- <property name="dao" ref="dao" />
		          					<property name="productProperties" ref="productProperties" /> -->
		          				</bean>
              				</property>
              			</bean>

             			<bean class="jobs.HappyBirthDayBonusProcessor">
          					<property name="dao" ref="dao" />
          					<property name="productProperties" ref="productProperties" />
          				</bean>
          			</util:list> 
      			</property> 
  			  </bean>
	          <!-- <bean class="jobs.HappyBirthDayBonusProcessor">
	          	<property name="dao" ref="dao" />
	          	<property name="productProperties" ref="productProperties" />
	          </bean> -->
	        </batch:processor>

	        <batch:writer>
	          <bean class="jobs.HappyBirthDayBonusWriter">
	          	<property name="i18n" ref="messageSource" />
	          	<property name="productProperties" ref="productProperties" />
	          </bean>
	        </batch:writer>

			<!-- Each stream element involved in the step.
			By default, objects referenced using a reader, processor, and writer are automatically registered. You don’t need to specify them again here. -->
	        <!-- <batch:streams>
	          <batch:stream ref="allHVCsWithNoBonusReader" />
	          <batch:stream ref="defaultBonusWriter" />
	        </batch:streams> -->

			<!-- The Step allows a limit for the number of times an exception can be skipped, and a list of exceptions that are 'skippable'. -->
         	<batch:skippable-exception-classes>
            	<batch:include class="exceptions.AirAvailabilityException" />

            	<!-- Skips validation exceptions -->
            	<!-- Because you set the filter property of the validating item processor to false (this is the default value), the item processor rethrows any ValidationException thrown by its validator.
            	This implies the configuration of a skip strategy if you don’t want to fail the whole job execution in case of a validation failure.
            	The skip configuration consists of setting a skip limit and skipping ValidationExceptions. -->
            	<!-- In my implementation, I were only to filter products that have a negative price, (I set the filter property of the ValidatingItemProcessor to true) and wouldn’t need any skip configuration.
            	But I keep this configuration to preserve the whole job execution failure whatever filter property is set to -->
            	<batch:include class="org.springframework.batch.item.validator.ValidationException" />

            	<!-- <batch:include class="org.springframework.batch.item.file.FlatFileParseException" />
            	<batch:exclude class="java.lang.Exception" />
            	<batch:exclude class="java.io.FileNotFoundException" /> -->
         	</batch:skippable-exception-classes>

			<!-- The Step allows a limit for the number of times an individual item can be retried, and a list of exceptions that are 'retryable'. -->
	        <batch:retryable-exception-classes>
	          	<batch:include class="org.springframework.dao.DeadlockLoserDataAccessException"/>
	        </batch:retryable-exception-classes>
	      </batch:chunk>

		  <!-- the Step can be configured with a list of exceptions that should not cause rollback. -->
	      <batch:no-rollback-exception-classes>
	         <batch:include class="java.lang.Throwable" />
	      </batch:no-rollback-exception-classes>

		  <!-- Intercepting Step Execution -->
		  <!-- Just as with the Job, there are many events during the execution of a Step where a user may need to perform some functionality.
		  This can be accomplished with one of many Step scoped listeners. -->
    	  <batch:listeners>
    	  	<batch:listener ref="jobRunListener" />

      		<batch:listener>
      			<bean class="jobs.listeners.StagingHappyBirthDayBonusSubscriberStepListener" scope="step">
      				<property name="dao" ref="dao" />
      				<property name="productProperties" ref="productProperties" />
      			</bean>
      		</batch:listener>
			<!-- Configuring the CustomerItemListener. It is useful when Logging Invalid Records as an example -->
    	    <!-- Logging Invalid Records -->
    	    <!-- While skipping problematic records is a useful tool, by itself it can raise an issue. In some scenarios, the 
			ability to skip a record is okay. Say you are mining data and come across something you can’t resolve; it’s 
			probably okay to skip it. However, when you get into situations where money is involved, say when 
			processing transactions, just skipping a record probably will not be a robust enough solution. In cases 
			like these, it is helpful to be able to log the record that was the cause of the error. In this section, you will 
			look at using an ItemListener to record records that were invalid.  

			 <batch:skippable-exception-classes>
            	<batch:exclude class="java.lang.Exception" />
         	</batch:skippable-exception-classes>

			If you use the fixed length record job as an example and execute it with a file that contains an input 
			record longer than 63 characters (FlatFileItemReader example with a threshold FixedLengthTokenizer), an exception (FlatFileParseException) will be thrown. However, since you have configured your 
			job to skip all exceptions that extend Exception, the exception will not affect your job’s results, yet your 
			happyBirthDayBonusSubscriberItemLogger will be called and log the item as required. -->
      		<batch:listener>
      			<!-- <bean id="happyBirthDayBonusSubscriberItemLogger" class="jobs.listeners.HappyBirthDayBonusSubscriberItemListener" scope="step"> -->
      			<bean id="happyBirthDayBonusSubscriberItemLogger" class="jobs.listeners.HappyBirthDayBonusSubscriberItemListener">
      				<property name="dao" ref="dao" />
      				<property name="productProperties" ref="productProperties" />
      			</bean>
      		</batch:listener>

			<!-- Configuring the CustomerSkipListener -->
    	    <!-- it is less global than CustomerItemListener and treats only item skipped. It is useful when Logging Invalid Records as an example -->
			<batch:listener ref="itemProcessingSkipListener" />

			<!-- Configuring the CustomerItemReadListener -->
    	    <!-- it is a CustomerItemListener restricted to only read methods. It is useful when Logging Invalid Records as an example -->
      		<batch:listener>
      			<bean class="jobs.listeners.HappyBirthDayBonusSubscriberItemReadListener" scope="step" />
      		</batch:listener>

			<!-- Configuring the CustomerItemWriteListener -->
    	    <!-- it is a CustomerItemListener restricted to only write methods -->
      		<batch:listener>
      			<bean class="jobs.listeners.StagingHappyBirthDayBonusProcessedSubscriberChunkUpdater" scope="step" />
      		</batch:listener>
    	  </batch:listeners>
	    </batch:tasklet>

		<!-- Declares job should end at this point, without the possibility of restart. BatchStatus will be COMPLETED. ExitStatus is configurable. -->
		<!-- The 'end' element instructs a Job to stop with a BatchStatus of COMPLETED. A Job that has finished with status COMPLETED cannot be restarted (the framework will throw a JobInstanceAlreadyCompleteException) -->
		<!-- The 'end' element also allows for an optional 'exit-code' attribute that can be used to customize the ExitStatus of the Job. If no 'exit-code' attribute is given, then the ExitStatus will be "COMPLETED" by default, to match the BatchStatus. -->
    	<!-- <batch:end on="FAILED" /> -->
    	<!-- <batch:end on="COMPLETED WITH SKIPS" to="errorPrint1" /> -->

		<!-- Declares job should fail at this point. BatchStatus will be FAILED. ExitStatus is configurable. -->
		<!-- The 'fail' element instructs a Job to stop with a BatchStatus of FAILED. Unlike the 'end' element, the 'fail' element will not prevent the Job from being restarted.
		The 'fail' element also allows for an optional 'exit-code' attribute that can be used to customize the ExitStatus of the Job. If no 'exit-code' attribute is given, then the ExitStatus will be "FAILED" by default, to match the BatchStatus. -->
		<!-- <batch:fail on="FAILED" exit-code="EARLY TERMINATION" /> -->

		<!-- Declares job should be stop at this point and provides pointer where execution should continue when the job is restarted. -->
		<!-- The 'stop' element instructs a Job to stop with a BatchStatus of STOPPED. Stopping a Job can provide a temporary break in processing so that the operator can take some action before restarting the Job. -->
		<!-- The 'stop' element requires a 'restart' attribute that specifies the step where execution should pick up when the Job is restarted. -->
		<!-- <batch:stop on="COMPLETED" restart="step2" /> -->

		<!-- If transition elements are used, then all of the behavior for the Step's transitions must be defined explicitly. While there is no limit to the number of transition elements on a Step, if the Step's execution results in an ExitStatus that is not covered by an element, then the framework will throw an exception and the Job will fail. The framework will automatically order transitions from most specific to least specific. -->
		<!-- Defines a transition from this step to the next one depending on the value of the exit status. ExitStatus represents the status of a Step after it finishes execution. More specifically, the 'next' element above references the exit code of the ExitStatus -->
    	<!-- <batch:next on="COMPLETED WITH SKIPS" to="errorPrint1" /> -->
    	<!-- <batch:next on="*" to="step2" /> -->
	  </batch:step>
	</batch:job>

	<!-- Job scope, introduced in Spring Batch 3.0 is similar to Step scope in configuration but is a Scope for the Job context so there is only one instance of such a bean per executing job.
	Additionally, support is provided for late binding of references accessible from the JobContext using #{..} placeholders. Using this feature, bean properties can be pulled from the job or job execution context and the job parameters. -->
	<!-- Because it is not part of the Spring container by default, the scope must be added explicitly, either by using the batch namespace:
	Or by including a bean definition explicitly for the JobScope <bean class="org.springframework.batch.core.scope.JobScope" /> (but not both): -->
	<bean id="jobPhaseEventListener" class="jobs.listeners.JobPhaseEventListener" scope="job">
		<property name="productProperties" ref="productProperties" />

    	<!-- <property name="name" value="#{jobParameters['input']}" /> -->
    	<!-- <property name="name" value="#{jobExecutionContext['input.name']}.txt" /> -->
	</bean>

	<!-- Using a scope of Step is required in order to use late binding since the bean cannot actually be instantiated until the Step starts, which allows the attributes to be found.
	Because it is not part of the Spring container by default, the scope must be added explicitly, either by using the batch namespace:
	or by including a bean definition explicitly for the StepScope <bean class="org.springframework.batch.core.scope.StepScope" /> (but not both): -->
    <bean id="jobRunListener" class="jobs.listeners.JobRunListener" scope="step">
      	<property name="dao" ref="dao" />
      	<property name="productProperties" ref="productProperties" />

      	<!-- <property name="resource" value="#{stepExecutionContext['input.file.name']}" />
      	<property name="resource" value="#{jobExecutionContext['input.file.name']}" /> -->
    </bean>

	<bean id="itemProcessingSkipListener" class="jobs.listeners.ItemProcessingSkipListener" scope="step" />

	<!-- Configuring a thread-safe JdbcCursorItemReader with an indicator -->
	<!-- You start by configuring a SynchronizingItemReader bean to make the delegate item reader thread-safe. The synchronized item reader uses the delegate property to reference the delegate item reader.
	You then use the processed indicator column in the SQL statement to read data. A processed value of false causes the database to return only unprocessed rows.
	Finally, you  disable Spring Batch state management. This is the other requirement to make the item reader thread-safe (with the synchronization of the read method).
	But by doing that, you lose the reader’s restartability feature, because the item reader won’t know where it left off after a failure.
	Luckily, the process indicator is there to enable restartability: the reader reads only unprocessed items. The item writer then needs to flag the product as handled using the processed column and then write the item, as described in the following listing. -->
	<bean id="allMTNKIFSubscribersWithNoHappyBirthDayBonusReader" class="jobs.SynchronizingHappyBirthDayBonusSubscriberReader">
	  	<property name="delegate">
	  		<!-- reader must be evaluated for every scheduled execution of the given job. -->
	  		<!-- If you set the scope of the bean to step, then a new bean will be created every time the step is executed. -->
			<bean class="jobs.HappyBirthDayBonusSubscriberReader" scope="step">
				<constructor-arg index="0" type="int" value="2" />
		   		<!-- <constructor-arg index="1" type="java.lang.String" value="${AWS_SECRET_ACCESS_KEY}" /> -->

			  	<property name="dataSource" ref="db_connection" />
			  	<property name="saveState" value="false" />
			  	<property name="rowMapper">
			    	<bean class="dao.mapping.HappyBirthDayBonusSubscriberRowMapper" />
			  	</property>
			  	<!-- <property name="sql">
			  		<value>
			    		SELECT ID,MSISDN,NAME,LANGUAGE,BIRTH_DATE,ASPU,BONUS,BONUS_EXPIRES_IN,LAST_UPDATE_TIME FROM MTN_KIF_BIRTHDAY_BONUS_EBA WHERE ((BONUS IS NULL) AND (ASPU IS NOT NULL) AND (LOCKED = 0))
			  		</value>
				</property> -->
			</bean>
	  	</property>
	</bean>

	<!-- Configuring a thread-safe JdbcCursorItemReader with an indicator -->
	<!-- You start by configuring a SynchronizingItemReader bean to make the delegate item reader thread-safe. The synchronized item reader uses the delegate property to reference the delegate item reader.
	You then use the processed indicator column in the SQL statement to read data. A processed value of false causes the database to return only unprocessed rows.
	Finally, you  disable Spring Batch state management. This is the other requirement to make the item reader thread-safe (with the synchronization of the read method).
	But by doing that, you lose the reader’s restartability feature, because the item reader won’t know where it left off after a failure.
	Luckily, the process indicator is there to enable restartability: the reader reads only unprocessed items. The item writer then needs to flag the product as handled using the processed column and then write the item, as described in the following listing. -->
	<bean id="runningPAMSubscribers" class="jobs.SynchronizingSubscriberReader">
	  <property name="delegate">
  		<!-- reader must be evaluated for every scheduled execution of the given job. -->
  		<!-- If you set the scope of the bean to step, then a new bean will be created every time the step is executed. -->
		<!-- <bean class="org.springframework.batch.item.database.JdbcCursorItemReader" scope="step"> -->
		<bean class="jobs.RunningPAMSubscriberReader" scope="step">
			<constructor-arg index="0" type="int" value="0" />
			<!-- <constructor-arg index="0" type="int" value="1" /> -->

		  	<property name="dataSource" ref="db_connection" />
		  	<!-- <property name="sql">
		  		<value>
		    		SELECT ID,MSISDN,FLAG,CRBT,LAST_UPDATE_TIME,CRBT_NEXT_RENEWAL_DATE,LOCKED FROM MTN_KIF_MSISDN_EBA WHERE ((FLAG = 1) AND (LOCKED = 0))
		    		SELECT ID,MSISDN,FLAG,CRBT,LAST_UPDATE_TIME,CRBT_NEXT_RENEWAL_DATE,LOCKED FROM MTN_KIF_MSISDN_EBA WHERE FLAG = 1
		  		</value>
		  	</property> -->
		  	<property name="saveState" value="false" />
		  	<property name="rowMapper">
		    	<bean class="dao.mapping.SubscriberRowMapper" />
		  	</property>
		</bean>
	  </property>
	</bean>

	<!-- The configurable "task-executor" attribute is used to specify which TaskExecutor implementation should be used to execute the individual flows. The default is SyncTaskExecutor, but an asynchronous TaskExecutor is required to run the steps in parallel. Note that the job will ensure that every flow in the split completes before aggregating the exit statuses and transitioning. -->
	<!-- <bean id="ParallelStepsTaskExecutor" class="org.springframework.core.task.SimpleAsyncTaskExecutor">
	  Spring creates a threadpool of 10 threads, executing each chunk in a different thread or 10 chunks in parallel
	  <property name="concurrencyLimit" value="10" />
	  <property name="concurrencyLimit" value="3" />
	</bean> -->

	<!-- <task:executor id="executorWithCallerRunsPolicy" pool-size="5-25" queue-capacity="100" rejection-policy="CALLER_RUNS" /> -->
	<!-- <task:executor id="MultithreadedStepsTaskExecutor" pool-size="10" queue-capacity="25" rejection-policy="CALLER_RUNS" keep-alive="1800" /> -->
	<!-- <task:executor id="MultithreadedStepsTaskExecutor" pool-size="5-10" queue-capacity="25" rejection-policy="CALLER_RUNS" keep-alive="3600" /> -->

	<!-- <task:executor id="MultithreadedStepsTaskExecutor" pool-size="5-10" queue-capacity="25" rejection-policy="CALLER_RUNS" keep-alive="1800" /> -->
	<task:executor id="MultithreadedStepsTaskExecutor" pool-size="4-7" queue-capacity="20" rejection-policy="CALLER_RUNS" keep-alive="1800" />

	<!-- <bean id="MultithreadedStepsTaskExecutor" class="org.springframework.scheduling.concurrent.ThreadPoolTaskExecutor">
	    <property name="corePoolSize" value="5" />
	    <property name="maxPoolSize" value="10" />
	    <property name="queueCapacity" value="25" />
	</bean> -->
	<!-- <bean id="MultithreadedStepsTaskExecutor" class="org.springframework.scheduling.concurrent.ThreadPoolTaskExecutor">
  		<property name="corePoolSize" value="5" />
  		<property name="maxPoolSize" value="5" />
	</bean> -->

</beans>