<?xml version="1.0" encoding="UTF-8"?>
<beans xmlns="http://www.springframework.org/schema/beans"
	xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
	xmlns:util="http://www.springframework.org/schema/beans"
	xmlns:batch="http://www.springframework.org/schema/batch"
	xmlns:task="http://www.springframework.org/schema/task"
	xmlns:context="http://www.springframework.org/schema/context"
	xmlns:p="http://www.springframework.org/schema/p"
	xsi:schemaLocation="http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context-4.1.xsd http://www.springframework.org/schema/util http://www.springframework.org/schema/util/spring-util-4.1.xsd http://www.springframework.org/schema/batch http://www.springframework.org/schema/batch/spring-batch-3.0.xsd http://www.springframework.org/schema/task http://www.springframework.org/schema/task/spring-task-4.1.xsd">

    <context:annotation-config />
    <context:component-scan base-package="jobs" />

	<bean id="myAsyncUncaughtExceptionHandler" class="exceptions.MyAsyncUncaughtExceptionHandler" />

	<task:annotation-driven executor="batchTaskExecutor" scheduler="scheduling" exception-handler="myAsyncUncaughtExceptionHandler" />

	<!-- By default when specifying @Async on a method, the executor that will be used is the one supplied to the 'annotation-driven' -->
	<!-- the value attribute of the @Async annotation can be used when needing to indicate that an executor other than the default should be used -->
	<!-- set up a default executor -->
	<!-- <task:executor id="batchTaskExecutor" pool-size="7" queue-capacity="10" rejection-policy="CALLER_RUNS" keep-alive="3600" /> -->
	<task:executor id="batchTaskExecutor" pool-size="03" queue-capacity="05" rejection-policy="CALLER_RUNS" keep-alive="3600" />
	<!-- <task:executor id="executorWithPoolSizeRange" pool-size="5-25" queue-capacity="100" rejection-policy="DISCARD" keep-alive="3600" /> -->
	<task:scheduler id="scheduling" pool-size="03" />

	<task:scheduled-tasks scheduler="scheduling">
		<!-- new CronTrigger("* 15 9-17 * * MON-FRI") -->
		<!-- java.lang.IllegalArgumentException: Cron expression must consist of 6 fields (found 7 in "0 00 0 1 APR,MAY,JUN ? 2018") -->

		<!-- <task:scheduled ref="jobs" method="run_database_maintenance" cron="0 30 01 ? * *"  /> -->
  		<!-- <task:scheduled ref="jobs" method="run_pam" cron="0 41 11 ? * *" /> -->
  		<!-- <task:scheduled ref="jobs" method="run_pam" cron="30 01 00 ? * *" /> -->
  		<task:scheduled ref="jobs" method="run_pam" cron="30 01/12 * ? * *" />

  		<!-- <task:scheduled ref="jobs" method="renew_crbt" cron="0 30 02 ? * *" /> -->
  		<!-- <task:scheduled ref="jobs" method="renew_crbt" cron="0 5/12 * 5/1 * ?" /> -->
  		<task:scheduled ref="jobs" method="runPeriodicSubscriberManagement" cron="0 5/12 * 5/1 * ?" />

		<!-- <task:scheduled ref="jobs" method="setHappyBirthDayBonus" cron="0 30 07 ? * *" /> -->
		<task:scheduled ref="jobs" method="setHappyBirthDayBonus" cron="0 03/12 * 5/1 * ?" />

  		<!-- <task:scheduled ref="jobs" method="clear_ussd" fixed-delay="900000" /> -->
  		<task:scheduled ref="jobs" method="clear_ussd" fixed-delay="720000" />
	</task:scheduled-tasks>

    <bean id="transactionManager" class="org.springframework.batch.support.transaction.ResourcelessTransactionManager" />
    <bean id="jobRepository" class="org.springframework.batch.core.repository.support.MapJobRepositoryFactoryBean" p:transactionManager-ref="transactionManager" />
    <bean id="jobLauncher" class="org.springframework.batch.core.launch.support.SimpleJobLauncher" p:jobRepository-ref="jobRepository" p:taskExecutor-ref="batchTaskExecutor" />
 	<!-- <bean id="jobLauncher" class="org.springframework.batch.core.launch.support.SimpleJobLauncher" p:jobRepository-ref="jobRepository">
	  	<property name="taskExecutor">
	    	<bean class="org.springframework.core.task.SimpleAsyncTaskExecutor" />
	  	</property>
  	</bean> -->

	<batch:job id="cleanExpiredUssdRequestJob">
	  <batch:step id="cleanUssdRequest">
	  	<!-- <batch:tasklet ref="cleanExpiredUssdRequestTasklet" transaction-manager="transactionManager" /> -->
	  	<batch:tasklet transaction-manager="transactionManager">
          <bean class="jobs.CleanExpiredUssdRequestAndMonitoringTasklet">
     		<property name="productProperties" ref="productProperties" />
     		<property name="dao" ref="dao" />
     	  </bean>
	  	</batch:tasklet>
	  </batch:step>
	</batch:job>

	<batch:job id="runningPAMJob">
	  <batch:listeners>
      	<!-- <batch:listener>
          <bean class="jobs.listeners.JobPhaseEventListener" />
        </batch:listener> -->
        <batch:listener ref="jobPhaseEventListener" />
  	  </batch:listeners>

	  <batch:step id="runningPAM">
	  	<!-- throttle-limit, this attribute configures the level of thread concurrency and has a default value of 4 -->
	  	<!-- Ensure the core pool size is larger than this limit. -->
	  	<!-- <batch:tasklet task-executor="MultithreadedStepsTaskExecutor" throttle-limit="5"> -->
	  	<batch:tasklet task-executor="MultithreadedStepsTaskExecutor" throttle-limit="4">
	  	<!-- <batch:tasklet> -->
	      <!-- <batch:chunk reader="subscribers" processor="runningPAMProcessor" writer="runningPAMWriter" commit-interval="100" /> -->
	      <!-- <batch:chunk reader="subscribers" processor="runningPAMProcessor" writer="runningPAMWriter" commit-interval="50" /> -->
	      <!-- <batch:chunk reader="subscribers" processor="runningPAMProcessor" writer="runningPAMWriter" commit-interval="100" skip-limit="10" retry-limit="3"> -->
	      <!-- <batch:chunk reader="subscribers" commit-interval="100" skip-limit="25" retry-limit="3"> -->
	      <!-- <batch:chunk reader="runningPAMSubscribers" commit-interval="20" skip-policy="skipPolicy" retry-limit="3"> -->
	      <batch:chunk reader="runningPAMSubscribers" commit-interval="20" skip-limit="20" retry-limit="3">

	        <batch:processor>
	    	  <!-- CompositeItemProcessor -->
	    	  <!-- You break up a step into three phases (reading, processing, and writing) to divide responsibilities 
					between components.  However, the business logic that needs to be applied to a given item may not 
					make sense to couple into a single ItemProcessor.  Spring Batch allows you to maintain that same 
					division of responsibilities within your business logic by chaining ItemProcessors within a step.  In this 
					section, you will look at how to chain ItemProcessors within a single step using Spring Batch’s 
					CompositeItemProcessor.

					The org.springframework.batch.item.support.CompositeItemProcessor is an implementation of the 
					ItemProcessor interface that delegates processing to each of a list of ItemProcessor implementations in 
					order.  As each processor returns its result, that result is passed onto the next processor until they all 
					have been called.  This pattern occurs regardless of the types returned so if the first ItemProcessor takes 
					a String as input it can return a Product object as output as long as the next ItemProcessor takes a 
					Product as input.  At the end, the result is passed to the ItemWriter configured for the step.  It is 
					important to note that just like any other ItemProcessor, if any of the processors this one delegates to 
					returns null, the item will not be process further.  Figure 8-2 shows how the processing within the 
					CompositeItemProcessor occurs. 

					<bean id="completeItemProcessor" class="org.springframework.batch.item.support.CompositeItemProcessor"> 
        				<property name="delegates"> 
            				<util:list> 
                				<ref bean="customerIdItemProcessor"/> 
                				<ref bean="accountExecutiveItemProcessor"/> 
            				</util:list> 
        				</property> 
    				</bean>
    				
    				we can combine in this manner the run pam and notification sms steps. But because of synchronization (be sure DA is provided and already available after pam running before reading DA) and faster granting bonus to subscribers (priority is subscriber get night advantages bonus as soon and fast as possible and not send sms notification) reasons, we separate processing into two phases or steps
			  -->
	          <bean class="jobs.RunningPAMProcessor" scope="step">
      			<property name="waitingForResponse" value="false" />
      			<property name="productProperties" ref="productProperties" />
      		  </bean>
	        </batch:processor>

	        <batch:writer>
	          <bean class="jobs.RunningPAMWriter">
	          	<property name="waitingForResponse" value="false" />
	          	<property name="dao" ref="dao" />
	          </bean>
	        </batch:writer>

			<!-- The Step allows a limit for the number of times an exception can be skipped, and a list of exceptions that are 'skippable'. -->
         	<!-- Skipping instead of failing -->
         	<!-- The job fails as soon as you reach 20 skipped products, as defined in the configuration -->
         	<batch:skippable-exception-classes>
            	<batch:include class="org.springframework.dao.DeadlockLoserDataAccessException" />
            	<batch:include class="exceptions.AirAvailabilityException" />

            	<!-- <batch:include class="org.springframework.batch.item.file.FlatFileParseException" />
            	<batch:exclude class="java.lang.Exception" />
            	<batch:exclude class="java.io.FileNotFoundException" /> -->
         	</batch:skippable-exception-classes>

			<!-- The Step allows a limit for the number of times an individual item can be retried, and a list of exceptions that are 'retryable'. -->
			<!-- Automatic retry in a chunk-oriented step can make jobs more robust. It’s a shame to fail a step because of an unstable network, when retrying a few milliseconds later could have worked.
			You now know about the default retry configuration in Spring Batch, and this should be enough for most cases. -->
			<!-- Spring Batch only retries the item processing and item writing phases. By default, a retryable exception triggers a rollback, so you should be careful because retrying too many times for too many items can degrade performance.
			You should use retryable exception only for exceptions that are nondeterministic, not for exceptions related to format or constraint violations, which are typically deterministic.
			
			Spring Batch retries only for exceptions thrown during item processing or item writing. Retry triggers a rollback, so retrying is costly : don’t abuse it!
			Note that Spring Batch doesn’t read the items again, by default, because it maintains a chunk-scoped cache. -->
			<!-- Override equals() and hashCode() when using retry
			In a chunk-oriented step, Spring Batch handles retry on the item processing and writing phases.
			By default, a retry implies a rollback, so Spring Batch must restore the context of retried operations across transactions.
			It needs to track items closely to know which item could have triggered the retry.
			Remember that Spring Batch can’t always know which item triggers an exception during the writing phase, because an item writer handles a list of items.
			Spring Batch relies on the identity of items to track them, so for Spring Batch retry to work correctly, you should override the equals and hashCode methods of your items’ classes—by using a database identifier, for example. -->
	        <batch:retryable-exception-classes>
	          	<!-- <batch:include class="org.springframework.dao.OptimisticLockingFailureException" /> -->
	          	<batch:include class="org.springframework.dao.DeadlockLoserDataAccessException" />
	        </batch:retryable-exception-classes>

			<!-- Registers retry listener -->
			<!-- Spring Batch provides the RetryListener interface to react to any retried operation. A retry listener can be useful to log retried operations and to gather information.
			Once you know more about transient failures, you’re more likely to change the system to avoid them in subsequent executions (remember, retried operations always degrade performance). -->
			<!-- Any time you need to know about retried operations—for example, to get rid of them! Spring Batch lets you register retry listeners to log errors.
			Retry is a built-in feature of chunk-oriented steps. What can you do if you need to retry in your own code, for example, in a tasklet? => Retrying in application code with the RetryTemplate -->
        	<batch:retry-listeners>
          		<batch:listener ref="retryListener" />
        	</batch:retry-listeners>

			<!-- You can combine retry with skip: a job retries an unsuccessful operation several times and then skips it.
			Remember that once Spring Batch reaches the retry limit, the exception causes the step to exit and, by default, fail.
			Use combined retry and skip when you don’t want a persisting transient error to fail a step. -->
	      </batch:chunk>

		  <!-- Avoiding a rollback for an exception -->
		  <!-- In a chunk-oriented step, Spring Batch rolls back a chunk transaction if an error occurs in the item processor or in the item writer.
		  This seems safe because an error could have corrupted the state of the transaction, so a rollback ensures data isn’t in an inconsistent state.
	      Sometimes you’re sure that a specific error didn’t corrupt the transaction, so Spring Batch can retry the operation or skip the item.
		  This saves a rollback and therefore a new transaction. Having fewer transactions is better because transactions are costly.
		  Use the no-rollback-exception-classes feature only when you’re sure that an exception can’t corrupt a transaction; consider yourself warned! -->
		  <!-- the Step can be configured with a list of exceptions that should not cause rollback. -->
		  <!-- Spring Batch doesn’t drive a chunk-oriented step the same way when a skippable exception is thrown in the reading, processing, or writing phase.
			
		  When an item reader throws a skippable exception, Spring Batch just calls the read method again on the item reader to get the next item. There’s no rollback on the transaction.
			
		  When an item processor throws a skippable exception, Spring Batch rolls back the transaction of the current chunk and resubmits the read items to the item processor, except for the one that triggered the skippable exception in the previous run.
			
		  When the item writer throws a skippable exception, because Spring Batch doesn’t know which item threw the exception, it reprocesses each item in the chunk one by one, in its own transaction.
	      When a writer throws a skippable exception, Spring Batch can’t know which item triggered the exception. Spring Batch then rolls back the transaction and processes the chunk item by item.
		  Note that Spring Batch doesn’t read the items again, by default, because it maintains a chunk-scoped cache.
		  -->
	      <batch:no-rollback-exception-classes>
	      	 <batch:include class="java.lang.Exception" />
	         <batch:include class="java.lang.Throwable" />
	      </batch:no-rollback-exception-classes>

		  <!-- Intercepting Step Execution -->
		  <!-- Just as with the Job, there are many events during the execution of a Step where a user may need to perform some functionality.
		  This can be accomplished with one of many Step scoped listeners. -->
    	  <batch:listeners>
    	  	<batch:listener ref="jobRunListener" />

      		<batch:listener>
      			<bean class="jobs.listeners.StagingRunningPAMStepListener" scope="step">
      				<property name="dao" ref="dao" />
      				<property name="productProperties" ref="productProperties" />
      			</bean>
      		</batch:listener>

			<!-- Configuring the CustomerItemListener. It is useful when Logging Invalid Records as an example -->
    	    <!-- Logging Invalid Records -->
    	    <!-- While skipping problematic records is a useful tool, by itself it can raise an issue. In some scenarios, the 
			ability to skip a record is okay. Say you are mining data and come across something you can’t resolve; it’s 
			probably okay to skip it. However, when you get into situations where money is involved, say when 
			processing transactions, just skipping a record probably will not be a robust enough solution. In cases 
			like these, it is helpful to be able to log the record that was the cause of the error. In this section, you will 
			look at using an ItemListener to record records that were invalid.  

			 <batch:skippable-exception-classes>
            	<batch:exclude class="java.lang.Exception" />
         	</batch:skippable-exception-classes>

			If you use the fixed length record job as an example and execute it with a file that contains an input 
			record longer than 63 characters (FlatFileItemReader example with a threshold FixedLengthTokenizer), an exception (FlatFileParseException) will be thrown. However, since you have configured your 
			job to skip all exceptions that extend Exception, the exception will not affect your job’s results, yet your 
			subscriberItemLogger will be called and log the item as required. -->
      		<batch:listener>
      			<!-- <bean id="subscriberItemLogger" class="jobs.listeners.SubscriberItemListener" scope="step"> -->
      			<bean id="subscriberItemLogger" class="jobs.listeners.SubscriberItemListener">
      				<property name="dao" ref="dao" />
      				<property name="productProperties" ref="productProperties" />
      			</bean>
      		</batch:listener>

			<!-- Configuring the CustomerSkipListener -->
    	    <!-- it is less global than CustomerItemListener and treats only item skipped. It is useful when Logging Invalid Records as an example -->
			<batch:listener ref="itemProcessingSkipListener" />

      		<batch:listener>
      			<bean class="jobs.listeners.StagingRunningPAMChunkUpdater" scope="step" />
      		</batch:listener>
    	  </batch:listeners>
	    </batch:tasklet>

		<!-- Declares job should end at this point, without the possibility of restart. BatchStatus will be COMPLETED. ExitStatus is configurable. -->
		<!-- The 'end' element instructs a Job to stop with a BatchStatus of COMPLETED. A Job that has finished with status COMPLETED cannot be restarted (the framework will throw a JobInstanceAlreadyCompleteException) -->
		<!-- The 'end' element also allows for an optional 'exit-code' attribute that can be used to customize the ExitStatus of the Job. If no 'exit-code' attribute is given, then the ExitStatus will be "COMPLETED" by default, to match the BatchStatus. -->
    	<!-- <batch:end on="FAILED" /> -->
    	<!-- <batch:end on="COMPLETED WITH SKIPS" to="errorPrint1" /> -->

		<!-- Declares job should fail at this point. BatchStatus will be FAILED. ExitStatus is configurable. -->
		<!-- The 'fail' element instructs a Job to stop with a BatchStatus of FAILED. Unlike the 'end' element, the 'fail' element will not prevent the Job from being restarted.
		The 'fail' element also allows for an optional 'exit-code' attribute that can be used to customize the ExitStatus of the Job. If no 'exit-code' attribute is given, then the ExitStatus will be "FAILED" by default, to match the BatchStatus. -->
		<!-- <batch:fail on="FAILED" exit-code="EARLY TERMINATION" /> -->

		<!-- Declares job should be stop at this point and provides pointer where execution should continue when the job is restarted. -->
		<!-- The 'stop' element instructs a Job to stop with a BatchStatus of STOPPED. Stopping a Job can provide a temporary break in processing so that the operator can take some action before restarting the Job. -->
		<!-- The 'stop' element requires a 'restart' attribute that specifies the step where execution should pick up when the Job is restarted. -->
		<!-- <batch:stop on="COMPLETED" restart="step2" /> -->

    	<!-- Syntax for the step attribute on
    	Value/special character : String, Description : Exact value of the step exit status, Examples : COMPLETED,FAILED
    	Value/special character : *, Description : Matches 0 or more characters, Examples : * (matches any value) => COMPLETED* (matches COMPLETED and COMPLETED WITH SKIPS)
    	Value/special character : ?, Description : Matches exactly one character, Examples : C?T (matches CAT but not COUNT)

		Note that Spring Batch is smart enough to order transitions from the most to the least specific automatically.
		This means the order of the next tags in the configuration doesn’t matter; you can define transitions with wildcards first (less specific) and transitions with exact values last (more specific).

		WARNING : Be careful when transitioning to a step using the * special character. If the * matches the FAILED exit status (because there’s no more specific match), the next step is executed even if the current step fails.
		Perhaps this isn’t what you want; you may want to fail the job execution when a step fails. When using conditional transitions, you must handle failed steps yourself. -->

		<!-- If transition elements are used, then all of the behavior for the Step's transitions must be defined explicitly. While there is no limit to the number of transition elements on a Step, if the Step's execution results in an ExitStatus that is not covered by an element, then the framework will throw an exception and the Job will fail. The framework will automatically order transitions from most specific to least specific. -->
		<!-- Defines a transition from this step to the next one depending on the value of the exit status. ExitStatus represents the status of a Step after it finishes execution. More specifically, the 'next' element above references the exit code of the ExitStatus -->

    	<!-- <batch:next on="COMPLETED WITH SKIPS" to="errorPrint1" /> -->
    	<!-- <batch:next on="COMPLETED*" to="errorPrint1" /> -->
    	<!-- <batch:next on="*" to="step2" /> -->

    	<!-- <batch:next on="COMPLETED" to="nightAdvantagesNotificationThroughSms" /> -->
    	<batch:next on="COMPLETED*" to="nightAdvantagesNotificationThroughSms" />
	  </batch:step>

	  <batch:step id="nightAdvantagesNotificationThroughSms">
	  	<batch:tasklet task-executor="MultithreadedStepsTaskExecutor" throttle-limit="4">
	      <!-- <batch:chunk commit-interval="100" skip-limit="20" retry-limit="3"> -->
	      <!-- <batch:chunk commit-interval="50" skip-policy="skipPolicy" retry-limit="3"> -->
	      <batch:chunk commit-interval="50" skip-limit="20" retry-limit="3">
	        <batch:reader>
				<bean class="jobs.SynchronizingNightAdvantagesSubscriberReader">
				  <property name="delegate">
			  		<!-- reader must be evaluated for every scheduled execution of the given job. -->
			  		<!-- If you set the scope of the bean to step, then a new bean will be created every time the step is executed. -->
					<!-- <bean class="org.springframework.batch.item.database.JdbcCursorItemReader" scope="step"> -->
					<bean class="jobs.NightAdvantagesSubscriberReader" scope="step">
					  	<property name="dataSource" ref="db_connection" />
					  	<!-- <property name="sql">
					  		<value>
					    		SELECT ID,MSISDN,FLAG,CRBT,LAST_UPDATE_TIME,CRBT_NEXT_RENEWAL_DATE,LOCKED FROM MTN_KIF_MSISDN_EBA WHERE ((FLAG = 1) AND (LOCKED = 0))
					    		SELECT ID,MSISDN,FLAG,CRBT,LAST_UPDATE_TIME,CRBT_NEXT_RENEWAL_DATE,LOCKED FROM MTN_KIF_MSISDN_EBA WHERE FLAG = 1
					  		</value>
					  	</property> -->
					  	<property name="saveState" value="false" />
					  	<property name="rowMapper">
					    	<bean class="dao.mapping.PAMRunningReportingRowMapper" />
					  	</property>
					</bean>
				  </property>
				</bean>
	        </batch:reader>

	        <batch:processor>
	          <bean class="jobs.NightAdvantagesNotificationProcessor" scope="step">
	          	<property name="dao" ref="dao" />
	          	<property name="productProperties" ref="productProperties" />
	          	<property name="waitingForResponse" value="false" />
	          </bean>
	        </batch:processor>

	        <batch:writer>
	          <bean class="jobs.NightAdvantagesNotificationWriter">
	          	<property name="i18n" ref="messageSource" />
	          	<property name="productProperties" ref="productProperties" />
	          </bean>
	        </batch:writer>

			<!-- The Step allows a limit for the number of times an exception can be skipped, and a list of exceptions that are 'skippable'. -->
         	<!-- Skipping instead of failing -->
         	<!-- The job fails as soon as you reach 20 skipped products, as defined in the configuration -->
         	<batch:skippable-exception-classes>
            	<batch:include class="exceptions.AirAvailabilityException" />
            	<batch:include class="org.springframework.dao.DeadlockLoserDataAccessException" />

            	<!-- <batch:include class="org.springframework.batch.item.file.FlatFileParseException" />
            	<batch:exclude class="java.lang.Exception" />
            	<batch:exclude class="java.io.FileNotFoundException" /> -->
         	</batch:skippable-exception-classes>

			<!-- The Step allows a limit for the number of times an individual item can be retried, and a list of exceptions that are 'retryable'. -->
			<!-- Automatic retry in a chunk-oriented step can make jobs more robust. It’s a shame to fail a step because of an unstable network, when retrying a few milliseconds later could have worked.
			You now know about the default retry configuration in Spring Batch, and this should be enough for most cases. -->
			<!-- Spring Batch only retries the item processing and item writing phases. By default, a retryable exception triggers a rollback, so you should be careful because retrying too many times for too many items can degrade performance.
			You should use retryable exception only for exceptions that are nondeterministic, not for exceptions related to format or constraint violations, which are typically deterministic.
			
			Spring Batch retries only for exceptions thrown during item processing or item writing. Retry triggers a rollback, so retrying is costly : don’t abuse it!
			Note that Spring Batch doesn’t read the items again, by default, because it maintains a chunk-scoped cache. -->
			<!-- Override equals() and hashCode() when using retry
			In a chunk-oriented step, Spring Batch handles retry on the item processing and writing phases.
			By default, a retry implies a rollback, so Spring Batch must restore the context of retried operations across transactions.
			It needs to track items closely to know which item could have triggered the retry.
			Remember that Spring Batch can’t always know which item triggers an exception during the writing phase, because an item writer handles a list of items.
			Spring Batch relies on the identity of items to track them, so for Spring Batch retry to work correctly, you should override the equals and hashCode methods of your items’ classes—by using a database identifier, for example. -->
	        <batch:retryable-exception-classes>
	          	<!-- <batch:include class="org.springframework.dao.OptimisticLockingFailureException" /> -->
	          	<batch:include class="org.springframework.dao.DeadlockLoserDataAccessException" />
	        </batch:retryable-exception-classes>

			<!-- Registers retry listener -->
			<!-- Spring Batch provides the RetryListener interface to react to any retried operation. A retry listener can be useful to log retried operations and to gather information.
			Once you know more about transient failures, you’re more likely to change the system to avoid them in subsequent executions (remember, retried operations always degrade performance). -->
			<!-- Any time you need to know about retried operations—for example, to get rid of them! Spring Batch lets you register retry listeners to log errors.
			Retry is a built-in feature of chunk-oriented steps. What can you do if you need to retry in your own code, for example, in a tasklet? => Retrying in application code with the RetryTemplate -->
        	<batch:retry-listeners>
          		<batch:listener ref="retryListener" />
        	</batch:retry-listeners>

			<!-- You can combine retry with skip: a job retries an unsuccessful operation several times and then skips it.
			Remember that once Spring Batch reaches the retry limit, the exception causes the step to exit and, by default, fail.
			Use combined retry and skip when you don’t want a persisting transient error to fail a step. -->
	      </batch:chunk>

		  <!-- Avoiding a rollback for an exception -->
		  <!-- In a chunk-oriented step, Spring Batch rolls back a chunk transaction if an error occurs in the item processor or in the item writer.
		  This seems safe because an error could have corrupted the state of the transaction, so a rollback ensures data isn’t in an inconsistent state.
	      Sometimes you’re sure that a specific error didn’t corrupt the transaction, so Spring Batch can retry the operation or skip the item.
		  This saves a rollback and therefore a new transaction. Having fewer transactions is better because transactions are costly.
		  Use the no-rollback-exception-classes feature only when you’re sure that an exception can’t corrupt a transaction; consider yourself warned! -->
		  <!-- the Step can be configured with a list of exceptions that should not cause rollback. -->
		  <!-- Spring Batch doesn’t drive a chunk-oriented step the same way when a skippable exception is thrown in the reading, processing, or writing phase.
			
		  When an item reader throws a skippable exception, Spring Batch just calls the read method again on the item reader to get the next item. There’s no rollback on the transaction.
			
		  When an item processor throws a skippable exception, Spring Batch rolls back the transaction of the current chunk and resubmits the read items to the item processor, except for the one that triggered the skippable exception in the previous run.
			
		  When the item writer throws a skippable exception, because Spring Batch doesn’t know which item threw the exception, it reprocesses each item in the chunk one by one, in its own transaction.
	      When a writer throws a skippable exception, Spring Batch can’t know which item triggered the exception. Spring Batch then rolls back the transaction and processes the chunk item by item.
		  Note that Spring Batch doesn’t read the items again, by default, because it maintains a chunk-scoped cache.
		  -->
	      <batch:no-rollback-exception-classes>
	      	 <batch:include class="java.lang.Exception" />
	         <batch:include class="java.lang.Throwable" />
	      </batch:no-rollback-exception-classes>

		  <!-- Intercepting Step Execution -->
		  <!-- Just as with the Job, there are many events during the execution of a Step where a user may need to perform some functionality.
		  This can be accomplished with one of many Step scoped listeners. -->
    	  <batch:listeners>
    	  	<batch:listener ref="jobRunListener" />
			<batch:listener ref="itemProcessingSkipListener" />
    	  </batch:listeners>
	    </batch:tasklet>
	  </batch:step>
	</batch:job>

	<!-- <batch:job id="crbtRenewalJob">
	  <batch:listeners>
      	<batch:listener>
          <bean class="jobs.listeners.JobPhaseEventListener" />
        </batch:listener>
        <batch:listener ref="jobPhaseEventListener" />
  	  </batch:listeners>

	  <batch:step id="crbtRenewal">
	  	<batch:tasklet ref="crbtRenewalTasklet" transaction-manager="transactionManager" />
	  	<batch:tasklet transaction-manager="transactionManager">
          <bean class="jobs.DefaultCrbtSongRenewalTasklet">
          	<property name="i18n" ref="messageSource" />
     		<property name="productProperties" ref="productProperties" />
     		<property name="dao" ref="dao" />
     	  </bean>
		  Intercepting Step Execution
		  Just as with the Job, there are many events during the execution of a Step where a user may need to perform some functionality.
		  This can be accomplished with one of many Step scoped listeners.
    	  <batch:listeners>
    	  	<batch:listener ref="validatingMonthlySubscriberManagementListener" />
    	  	<batch:listener ref="jobRunListener" />
    	  </batch:listeners>
	    </batch:tasklet>
	  </batch:step>
	</batch:job> -->

	<batch:job id="periodicSubscriberManagementJob">
	  <batch:listeners>
      	<!-- <batch:listener>
          <bean class="jobs.listeners.JobPhaseEventListener" />
        </batch:listener> -->
        <batch:listener ref="jobPhaseEventListener" />
  	  </batch:listeners>

	  <batch:step id="defaultCrbtSongRenewalDataCalculating">
	  	<batch:tasklet transaction-manager="transactionManager">
          <bean class="jobs.DefaultCrbtSongRenewalDataCalculatingTasklet">
     		<property name="productProperties" ref="productProperties" />
     		<property name="dao" ref="dao" />
     	  </bean>
		  <!-- Intercepting Step Execution -->
		  <!-- Just as with the Job, there are many events during the execution of a Step where a user may need to perform some functionality.
		  This can be accomplished with one of many Step scoped listeners. -->
    	  <batch:listeners>
    	  	<batch:listener ref="validatingMonthlySubscriberManagementListener" />

			<batch:listener ref="jobRunListener" />

    	  	<!-- Sharing data between steps : Promoting data from the step to the job execution context -->
    	  	<!-- USING THE STEP EXECUTION CONTEXT AND PROMOTING THE DATA TO THE JOB EXECUTION CONTEXT -->
			<!-- The solution used to promote data from a step’s scope to a job’s scope consists of two parts: you programmatically make the data available in the step execution context and promote this data to the job execution context through configuration.
			This solution  looks complex, but it confines the data exposure to the step and makes the choice of exposing the data a configuration choice.
			The communicating steps aren’t as tightly coupled with each other as in the pure job execution context solution. Imagine that you write jobs by assembling existing tasklets.
			In some cases, you need to share data between steps, and in other cases, you don’t need to share data. By using the  promotion approach, you can choose to conceal data from the step execution context or to expose the data transparently only when you need to. -->
    	  	<!-- How does the data end up in the job execution context? Spring Batch provides a ready-to-use step listener for that, the ExecutionContextPromotionListener.
			All you need to do is register the step listener on the writing step and set which key(s) you want to promote. -->
    	  	<!-- Registers listener on writing step : Declares promotion step listener -->
    	  	<batch:listener>
    	  		<!-- Declares promotion step listener -->
				<bean id="promotionListener" class="org.springframework.batch.core.listener.ExecutionContextPromotionListener" scope="job">
				  <property name="keys" value="allMSISDN_With_ASPU_NotReachedFlag" />
				</bean>
			</batch:listener>
    	  </batch:listeners>
	    </batch:tasklet>

		<!-- Declares job should end at this point, without the possibility of restart. BatchStatus will be COMPLETED. ExitStatus is configurable. -->
		<!-- The 'end' element instructs a Job to stop with a BatchStatus of COMPLETED. A Job that has finished with status COMPLETED cannot be restarted (the framework will throw a JobInstanceAlreadyCompleteException) -->
		<!-- The 'end' element also allows for an optional 'exit-code' attribute that can be used to customize the ExitStatus of the Job. If no 'exit-code' attribute is given, then the ExitStatus will be "COMPLETED" by default, to match the BatchStatus. -->
    	<batch:end on="FAILED" exit-code="STOPPED BECAUSE OF ERRORS" />
    	<batch:end on="STOPPED" exit-code="Job should not be run right now." />
    	<batch:end on="COMPLETED WITH NO DATA" />

    	<!-- Syntax for the step attribute on
    	Value/special character : String, Description : Exact value of the step exit status, Examples : COMPLETED,FAILED
    	Value/special character : *, Description : Matches 0 or more characters, Examples : * (matches any value) => COMPLETED* (matches COMPLETED and COMPLETED WITH SKIPS)
    	Value/special character : ?, Description : Matches exactly one character, Examples : C?T (matches CAT but not COUNT)

		Note that Spring Batch is smart enough to order transitions from the most to the least specific automatically.
		This means the order of the next tags in the configuration doesn’t matter; you can define transitions with wildcards first (less specific) and transitions with exact values last (more specific).

		WARNING : Be careful when transitioning to a step using the * special character. If the * matches the FAILED exit status (because there’s no more specific match), the next step is executed even if the current step fails.
		Perhaps this isn’t what you want; you may want to fail the job execution when a step fails. When using conditional transitions, you must handle failed steps yourself. -->

		<!-- If transition elements are used, then all of the behavior for the Step's transitions must be defined explicitly. While there is no limit to the number of transition elements on a Step, if the Step's execution results in an ExitStatus that is not covered by an element, then the framework will throw an exception and the Job will fail. The framework will automatically order transitions from most specific to least specific. -->
		<!-- Defines a transition from this step to the next one depending on the value of the exit status. ExitStatus represents the status of a Step after it finishes execution. More specifically, the 'next' element above references the exit code of the ExitStatus -->

    	<!-- <batch:next on="COMPLETED WITH SKIPS" to="errorPrint1" /> -->
    	<!-- <batch:next on="COMPLETED*" to="errorPrint1" /> -->
    	<!-- <batch:next on="*" to="step2" /> -->

    	<!-- <batch:next on="COMPLETED" to="periodicSubscriberManagement" /> -->
		<batch:next on="COMPLETED*" to="periodicSubscriberManagement" />
	  </batch:step>

	  <!-- A single step cannot have both a "next" attribute and a transition element. -->
	  <!-- <batch:step id="stepA" next="stepB" /> -->
	  <batch:step id="periodicSubscriberManagement">
	  	<batch:tasklet task-executor="MultithreadedStepsTaskExecutor" throttle-limit="4">
	      <batch:chunk reader="periodicSubscriberManagementReader" commit-interval="50" skip-limit="20" retry-limit="3">

	        <batch:processor>
	    	  <!-- CompositeItemProcessor -->
	    	  <!-- You break up a step into three phases (reading, processing, and writing) to divide responsibilities 
					between components.  However, the business logic that needs to be applied to a given item may not 
					make sense to couple into a single ItemProcessor.  Spring Batch allows you to maintain that same 
					division of responsibilities within your business logic by chaining ItemProcessors within a step.  In this 
					section, you will look at how to chain ItemProcessors within a single step using Spring Batch’s
					CompositeItemProcessor.

					The org.springframework.batch.item.support.CompositeItemProcessor is an implementation of the 
					ItemProcessor interface that delegates processing to each of a list of ItemProcessor implementations in 
					order.  As each processor returns its result, that result is passed onto the next processor until they all 
					have been called.  This pattern occurs regardless of the types returned so if the first ItemProcessor takes 
					a String as input it can return a Product object as output as long as the next ItemProcessor takes a 
					Product as input.  At the end, the result is passed to the ItemWriter configured for the step.  It is 
					important to note that just like any other ItemProcessor, if any of the processors this one delegates to 
					returns null, the item will not be process further.  Figure 8-2 shows how the processing within the 
					CompositeItemProcessor occurs. 
			  -->
			  <bean id="completePeriodicSubscriberManagementItemProcessor" class="org.springframework.batch.item.support.CompositeItemProcessor">
      			<property name="delegates"> 
          			<util:list> 
              			<bean class="jobs.MonthlyReminderProcessor">
          					<property name="i18n" ref="messageSource" />
          					<property name="productProperties" ref="productProperties" />
          				</bean>

             			<bean class="jobs.DefaultCrbtSongRenewalProcessor" scope="step">
          					<property name="dao" ref="dao" />
          					<property name="productProperties" ref="productProperties" />

          					<!-- Sharing data between steps : WRITING IN THE JOB EXECUTION CONTEXT AND READING USING LATE BINDING -->
          					<!-- This approach consists of writing in the job execution context and then referring to the data from the XML configuration, using late binding.
          					The receiving tasklet doesn’t read from any execution context: it reads its own property.
          					The good news about this new version of the receiving tasklet is that it doesn’t depend on the Spring Batch runtime anymore.
          					Speaking of dependency injection, the responsibility of setting the property correctly now belongs to Spring, with help from the step scope and SpEL.
          					The # character triggers the evaluation of an expression. This implicit variable is available only for a step-scoped Spring bean running in a step! Spring Batch creates the instance at the last moment, when the step is about to run. That’s why we call it late binding. -->
          					<property name="allMSISDN_With_ASPU_ReachedFlag" value="#{jobExecutionContext['allMSISDN_With_ASPU_ReachedFlag']}" />
          					<property name="allMSISDN_With_ASPU_NotReachedFlag" value="#{jobExecutionContext['allMSISDN_With_ASPU_NotReachedFlag']}" />
          				</bean>
          			</util:list>
      			</property>
  			  </bean>
	        </batch:processor>

	        <batch:writer>
	          <bean class="jobs.PeriodicSubscriberManagementWriter">
	          	<property name="i18n" ref="messageSource" />
	          	<property name="productProperties" ref="productProperties" />
	          </bean>
	        </batch:writer>

			<!-- Each stream element involved in the step.
			By default, objects referenced using a reader, processor, and writer are automatically registered. You don’t need to specify them again here. -->
	        <!-- <batch:streams>
	          <batch:stream ref="allHVCsWithNoBonusReader" />
	          <batch:stream ref="defaultBonusWriter" />
	        </batch:streams> -->

			<!-- The Step allows a limit for the number of times an exception can be skipped, and a list of exceptions that are 'skippable'. -->
         	<!-- Skipping instead of failing -->
         	<!-- The job fails as soon as you reach 20 skipped products, as defined in the configuration -->
         	<batch:skippable-exception-classes>
         		<batch:include class="org.springframework.dao.DeadlockLoserDataAccessException" />
            	<batch:include class="exceptions.AirAvailabilityException" />
            	<batch:exclude class="exceptions.HuaweiCrbtServerException" />
            	<!-- <batch:exclude class="java.lang.Exception" />
            	<batch:exclude class="java.lang.Throwable" /> -->

            	<!-- <batch:include class="org.springframework.batch.item.file.FlatFileParseException" />
            	<batch:exclude class="java.io.FileNotFoundException" /> -->
         	</batch:skippable-exception-classes>

			<!-- The Step allows a limit for the number of times an individual item can be retried, and a list of exceptions that are 'retryable'. -->
			<!-- Automatic retry in a chunk-oriented step can make jobs more robust. It’s a shame to fail a step because of an unstable network, when retrying a few milliseconds later could have worked.
			You now know about the default retry configuration in Spring Batch, and this should be enough for most cases. -->
			<!-- Spring Batch only retries the item processing and item writing phases. By default, a retryable exception triggers a rollback, so you should be careful because retrying too many times for too many items can degrade performance.
			You should use retryable exception only for exceptions that are nondeterministic, not for exceptions related to format or constraint violations, which are typically deterministic.
			
			Spring Batch retries only for exceptions thrown during item processing or item writing. Retry triggers a rollback, so retrying is costly : don’t abuse it!
			Note that Spring Batch doesn’t read the items again, by default, because it maintains a chunk-scoped cache. -->
			<!-- Override equals() and hashCode() when using retry
			In a chunk-oriented step, Spring Batch handles retry on the item processing and writing phases.
			By default, a retry implies a rollback, so Spring Batch must restore the context of retried operations across transactions.
			It needs to track items closely to know which item could have triggered the retry.
			Remember that Spring Batch can’t always know which item triggers an exception during the writing phase, because an item writer handles a list of items.
			Spring Batch relies on the identity of items to track them, so for Spring Batch retry to work correctly, you should override the equals and hashCode methods of your items’ classes—by using a database identifier, for example. -->
	        <batch:retryable-exception-classes>
	          	<!-- <batch:include class="org.springframework.dao.OptimisticLockingFailureException" /> -->
	          	<!-- <batch:include class="org.springframework.dao.DeadlockLoserDataAccessException" /> -->
	          	<batch:include class="org.springframework.dao.DeadlockLoserDataAccessException"/>
	          	<batch:exclude class="exceptions.HuaweiCrbtServerException" />
	        </batch:retryable-exception-classes>

			<!-- Registers retry listener -->
			<!-- Spring Batch provides the RetryListener interface to react to any retried operation. A retry listener can be useful to log retried operations and to gather information.
			Once you know more about transient failures, you’re more likely to change the system to avoid them in subsequent executions (remember, retried operations always degrade performance). -->
			<!-- Any time you need to know about retried operations—for example, to get rid of them! Spring Batch lets you register retry listeners to log errors.
			Retry is a built-in feature of chunk-oriented steps. What can you do if you need to retry in your own code, for example, in a tasklet? => Retrying in application code with the RetryTemplate -->
        	<batch:retry-listeners>
          		<batch:listener ref="retryListener" />
        	</batch:retry-listeners>

			<!-- You can combine retry with skip: a job retries an unsuccessful operation several times and then skips it.
			Remember that once Spring Batch reaches the retry limit, the exception causes the step to exit and, by default, fail.
			Use combined retry and skip when you don’t want a persisting transient error to fail a step. -->
	      </batch:chunk>

		  <!-- Avoiding a rollback for an exception -->
		  <!-- In a chunk-oriented step, Spring Batch rolls back a chunk transaction if an error occurs in the item processor or in the item writer.
		  This seems safe because an error could have corrupted the state of the transaction, so a rollback ensures data isn’t in an inconsistent state.
	      Sometimes you’re sure that a specific error didn’t corrupt the transaction, so Spring Batch can retry the operation or skip the item.
		  This saves a rollback and therefore a new transaction. Having fewer transactions is better because transactions are costly.
		  Use the no-rollback-exception-classes feature only when you’re sure that an exception can’t corrupt a transaction; consider yourself warned! -->
		  <!-- the Step can be configured with a list of exceptions that should not cause rollback. -->
		  <!-- Spring Batch doesn’t drive a chunk-oriented step the same way when a skippable exception is thrown in the reading, processing, or writing phase.
			
		  When an item reader throws a skippable exception, Spring Batch just calls the read method again on the item reader to get the next item. There’s no rollback on the transaction.
			
		  When an item processor throws a skippable exception, Spring Batch rolls back the transaction of the current chunk and resubmits the read items to the item processor, except for the one that triggered the skippable exception in the previous run.
			
		  When the item writer throws a skippable exception, because Spring Batch doesn’t know which item threw the exception, it reprocesses each item in the chunk one by one, in its own transaction.
	      When a writer throws a skippable exception, Spring Batch can’t know which item triggered the exception. Spring Batch then rolls back the transaction and processes the chunk item by item.
		  Note that Spring Batch doesn’t read the items again, by default, because it maintains a chunk-scoped cache.
		  -->
	      <batch:no-rollback-exception-classes>
	      	 <batch:include class="java.lang.Exception" />
	         <batch:include class="java.lang.Throwable" />
	      </batch:no-rollback-exception-classes>

		  <!-- Intercepting Step Execution -->
		  <!-- Just as with the Job, there are many events during the execution of a Step where a user may need to perform some functionality.
		  This can be accomplished with one of many Step scoped listeners. -->
    	  <batch:listeners>
    	  	<batch:listener ref="validatingMonthlySubscriberManagementListener" />
    	  	<batch:listener ref="jobRunListener" />
			<batch:listener ref="itemProcessingSkipListener" />
    	  </batch:listeners>
	    </batch:tasklet>

		<!-- Declares job should end at this point, without the possibility of restart. BatchStatus will be COMPLETED. ExitStatus is configurable. -->
		<!-- The 'end' element instructs a Job to stop with a BatchStatus of COMPLETED. A Job that has finished with status COMPLETED cannot be restarted (the framework will throw a JobInstanceAlreadyCompleteException) -->
		<!-- The 'end' element also allows for an optional 'exit-code' attribute that can be used to customize the ExitStatus of the Job. If no 'exit-code' attribute is given, then the ExitStatus will be "COMPLETED" by default, to match the BatchStatus. -->
    	<!-- <batch:end on="FAILED" /> -->
    	<!-- <batch:end on="COMPLETED WITH SKIPS" to="errorPrint1" /> -->

		<!-- Declares job should fail at this point. BatchStatus will be FAILED. ExitStatus is configurable. -->
		<!-- The 'fail' element instructs a Job to stop with a BatchStatus of FAILED. Unlike the 'end' element, the 'fail' element will not prevent the Job from being restarted.
		The 'fail' element also allows for an optional 'exit-code' attribute that can be used to customize the ExitStatus of the Job. If no 'exit-code' attribute is given, then the ExitStatus will be "FAILED" by default, to match the BatchStatus. -->
		<!-- <batch:fail on="FAILED" exit-code="EARLY TERMINATION" /> -->

		<!-- Declares job should be stop at this point and provides pointer where execution should continue when the job is restarted. -->
		<!-- The 'stop' element instructs a Job to stop with a BatchStatus of STOPPED. Stopping a Job can provide a temporary break in processing so that the operator can take some action before restarting the Job. -->
		<!-- The 'stop' element requires a 'restart' attribute that specifies the step where execution should pick up when the Job is restarted. -->
		<!-- <batch:stop on="COMPLETED" restart="step2" /> -->

    	<!-- Syntax for the step attribute on
    	Value/special character : String, Description : Exact value of the step exit status, Examples : COMPLETED,FAILED
    	Value/special character : *, Description : Matches 0 or more characters, Examples : * (matches any value) => COMPLETED* (matches COMPLETED and COMPLETED WITH SKIPS)
    	Value/special character : ?, Description : Matches exactly one character, Examples : C?T (matches CAT but not COUNT)

		Note that Spring Batch is smart enough to order transitions from the most to the least specific automatically.
		This means the order of the next tags in the configuration doesn’t matter; you can define transitions with wildcards first (less specific) and transitions with exact values last (more specific).

		WARNING : Be careful when transitioning to a step using the * special character. If the * matches the FAILED exit status (because there’s no more specific match), the next step is executed even if the current step fails.
		Perhaps this isn’t what you want; you may want to fail the job execution when a step fails. When using conditional transitions, you must handle failed steps yourself. -->

		<!-- If transition elements are used, then all of the behavior for the Step's transitions must be defined explicitly. While there is no limit to the number of transition elements on a Step, if the Step's execution results in an ExitStatus that is not covered by an element, then the framework will throw an exception and the Job will fail. The framework will automatically order transitions from most specific to least specific. -->
		<!-- Defines a transition from this step to the next one depending on the value of the exit status. ExitStatus represents the status of a Step after it finishes execution. More specifically, the 'next' element above references the exit code of the ExitStatus -->

    	<!-- <batch:next on="COMPLETED WITH SKIPS" to="errorPrint1" /> -->
    	<!-- <batch:next on="COMPLETED*" to="errorPrint1" /> -->
    	<!-- <batch:next on="*" to="step2" /> -->
	  </batch:step>
	</batch:job>

	<!-- To disable restart, set the attribute restartable to false on the job element:
	Remember that jobs are restartable by default. If you’re worried that you’ll forget that, set the restartable flag explicitly on all your jobs. -->
	<batch:job id="happyBirthDayBonusJob">
	  <batch:listeners>
      	<!-- <batch:listener>
          <bean class="jobs.listeners.JobPhaseEventListener" />
        </batch:listener> -->
        <batch:listener ref="jobPhaseEventListener" />
  	  </batch:listeners>

	  <!-- A single step cannot have both a "next" attribute and a transition element. -->
	  <!-- <batch:step id="stepA" next="stepB" /> -->
	  <batch:step id="happyBirthDayBonus">
	  	<!-- <batch:tasklet task-executor="MultithreadedStepsTaskExecutor" throttle-limit="5"> -->
	  	<batch:tasklet task-executor="MultithreadedStepsTaskExecutor" throttle-limit="4">
	      <!-- <batch:chunk reader="allMTNKIFSubscribersWithNoHappyBirthDayBonusReader" processor="happyBirthdayBonusProcessor" writer="happyBirthdayBonusWriter" commit-interval="10" skip-limit="20" retry-limit="3"> -->
	      <!-- <batch:chunk reader="allMTNKIFSubscribersWithNoHappyBirthDayBonusReader" commit-interval="10" skip-policy="skipPolicy" retry-limit="3"> -->
	      <batch:chunk reader="allMTNKIFSubscribersWithNoHappyBirthDayBonusReader" commit-interval="10" skip-limit="20" retry-limit="3">

	        <batch:processor>
	    	  <!-- CompositeItemProcessor -->
	    	  <!-- You break up a step into three phases (reading, processing, and writing) to divide responsibilities 
					between components.  However, the business logic that needs to be applied to a given item may not 
					make sense to couple into a single ItemProcessor.  Spring Batch allows you to maintain that same 
					division of responsibilities within your business logic by chaining ItemProcessors within a step.  In this 
					section, you will look at how to chain ItemProcessors within a single step using Spring Batch’s 
					CompositeItemProcessor.

					The org.springframework.batch.item.support.CompositeItemProcessor is an implementation of the 
					ItemProcessor interface that delegates processing to each of a list of ItemProcessor implementations in 
					order.  As each processor returns its result, that result is passed onto the next processor until they all 
					have been called.  This pattern occurs regardless of the types returned so if the first ItemProcessor takes 
					a String as input it can return a Product object as output as long as the next ItemProcessor takes a 
					Product as input.  At the end, the result is passed to the ItemWriter configured for the step.  It is 
					important to note that just like any other ItemProcessor, if any of the processors this one delegates to 
					returns null, the item will not be process further.  Figure 8-2 shows how the processing within the 
					CompositeItemProcessor occurs. 
			  -->
			  <bean id="completeItemProcessor" class="org.springframework.batch.item.support.CompositeItemProcessor">
      			<property name="delegates"> 
          			<util:list> 
              			<!-- <ref bean="customerIdItemProcessor"/> 
              			<ref bean="accountExecutiveItemProcessor"/> -->
              			
              			<!-- Validating items -->
              			<!-- Because validation is business logic, the standard location to enforce validation rules is in the item-processing phase of a chunk-oriented step.
              			A common practice in Spring Batch is for an item processor to perform validation checks on read items and decide whether to send the items to the item writer.
              			As an example, let’s see how to validate the price of imported products and check that prices aren’t negative numbers (products with  a  negative  price  shouldn’t  reach  the  database—you  don’t  want  to  credit  your customers!).
              			Should  you  consider  an  item  that  fails  the  validation  check  filtered  or skipped? Skipping is semantically closer to a validation failure, but this remains questionable, and the business requirements usually lead to the correct answer.
              			
              			A validation failure should lead to a skipped or filtered item, but what you care about is that the item writer doesn’t receive the item in question.
              			Remember that the corresponding step-execution metadata stored in the job repository is distinct (skip and filter count), and this distinction can be relevant for some use cases.
              			If you want to enforce validation rules in your item processor, use the following semantics for validation failure in the item processor’s process method:
              				If validation means skip, throw a runtime exception
              				If validation means filter, return null
              			
              			What kind of validation can an item processor perform? You can do almost anything: state validation of the read object, consistency check with other data, and so forth.
              			In the import products job, for example, you can check that the price of products from the flat file is positive.
              			A well-formatted negative price would pass the reading phase (no parsing exception), but you shouldn’t write the product to the database.
              			The job of the item processor is to enforce this check and discard invalid products.
              			You can implement an item processor corresponding to this example and follow the semantics outlined here, but Spring Batch already provides a class called ValidatingItemProcessor to handle this task.
              			
              			The Spring Batch class ValidatingItemProcessor has two interesting characteristics:
              				It  delegates  validation  to  an  implementation  of  the  Spring  Batch  Validator interface.
              				It has a filter flag that can be set to false to throw an exception (skip) or true  to  return  null  (filter)  if  the  validation  fails.  The  default  value  is  false (skip).
              			By using the ValidatingItemProcessor class, you can embed your validation rules in dedicated Validator implementations (which you can reuse) and choose your validation semantics by setting the filter property.
              			When you decide to use the ValidatingItemProcessor class, you can either code your validation logic in Validator implementations or create a Validator bridge to a full-blown validation framework. -->
              			<bean class="org.springframework.batch.item.validator.ValidatingItemProcessor">
              				<!-- If filter property is set to false (this is the default value), the item processor rethrows any ValidationException thrown by its validator to enforce skipping. This implies the configuration of a skip strategy if you don’t want to fail the whole job execution in case of a validation failure. -->
              				<!-- <property name="filter" value="false" /> -->

              				<!-- If filter property is set to true, item becomes null after validation phase and is filtered (not handled according the business rule) -->
              				<!-- If you were only to filter products that have a negative price, you would set the filter property of the ValidatingItemProcessor to true and wouldn’t need any skip configuration. -->
              				<property name="filter" value="true" />

              				<property name="validator">
		             			<bean class="jobs.HappyBirthDayBonusSubscriberValidator">
							   		<!-- <constructor-arg index="0" type="product.ProductProperties">
							   			<ref local="anotherExampleBean" />
							   			<ref bean="productProperties" />
							   		</constructor-arg> -->
		             				<constructor-arg index="0" type="product.ProductProperties" ref="productProperties" />
		             				<!-- <constructor-arg index="0" type="product.ProductPropertiesBasedOnPropertiesFactoryBean" ref="productProperties" /> -->
		             				<!-- <constructor-arg index="0" type="product.ProductPropertiesBasedOnPropertyPlaceholderConfigurer" ref="productProperties" /> -->
		             				<constructor-arg index="1" type="dao.DAO" ref="dao" />

		          					<!-- <property name="dao" ref="dao" />
		          					<property name="productProperties" ref="productProperties" /> -->
		          				</bean>
              				</property>
              			</bean>

             			<bean class="jobs.HappyBirthDayBonusProcessor">
          					<property name="dao" ref="dao" />
          					<property name="productProperties" ref="productProperties" />
          				</bean>
          			</util:list> 
      			</property> 
  			  </bean>
	          <!-- <bean class="jobs.HappyBirthDayBonusProcessor">
	          	<property name="dao" ref="dao" />
	          	<property name="productProperties" ref="productProperties" />
	          </bean> -->
	        </batch:processor>

	        <batch:writer>
	          <bean class="jobs.HappyBirthDayBonusWriter">
	          	<property name="i18n" ref="messageSource" />
	          	<property name="productProperties" ref="productProperties" />
	          </bean>
	        </batch:writer>

			<!-- Each stream element involved in the step.
			By default, objects referenced using a reader, processor, and writer are automatically registered. You don’t need to specify them again here. -->
	        <!-- <batch:streams>
	          <batch:stream ref="allHVCsWithNoBonusReader" />
	          <batch:stream ref="defaultBonusWriter" />
	        </batch:streams> -->

			<!-- The Step allows a limit for the number of times an exception can be skipped, and a list of exceptions that are 'skippable'. -->
         	<!-- Skipping instead of failing -->
         	<!-- The job fails as soon as you reach 20 skipped products, as defined in the configuration -->
         	<batch:skippable-exception-classes>
            	<batch:include class="exceptions.AirAvailabilityException" />
            	<batch:include class="org.springframework.dao.DeadlockLoserDataAccessException" />

            	<!-- Skips validation exceptions -->
            	<!-- Because you set the filter property of the validating item processor to false (this is the default value), the item processor rethrows any ValidationException thrown by its validator.
            	This implies the configuration of a skip strategy if you don’t want to fail the whole job execution in case of a validation failure.
            	The skip configuration consists of setting a skip limit and skipping ValidationExceptions. -->
            	<!-- In my implementation, I were only to filter products that have a negative price, (I set the filter property of the ValidatingItemProcessor to true) and wouldn’t need any skip configuration.
            	But I keep this configuration to preserve the whole job execution failure whatever filter property is set to -->
            	<batch:include class="org.springframework.batch.item.validator.ValidationException" />

            	<!-- <batch:include class="org.springframework.batch.item.file.FlatFileParseException" />
            	<batch:exclude class="java.lang.Exception" />
            	<batch:exclude class="java.io.FileNotFoundException" /> -->
         	</batch:skippable-exception-classes>

			<!-- The Step allows a limit for the number of times an individual item can be retried, and a list of exceptions that are 'retryable'. -->
			<!-- Automatic retry in a chunk-oriented step can make jobs more robust. It’s a shame to fail a step because of an unstable network, when retrying a few milliseconds later could have worked.
			You now know about the default retry configuration in Spring Batch, and this should be enough for most cases. -->
			<!-- Spring Batch only retries the item processing and item writing phases. By default, a retryable exception triggers a rollback, so you should be careful because retrying too many times for too many items can degrade performance.
			You should use retryable exception only for exceptions that are nondeterministic, not for exceptions related to format or constraint violations, which are typically deterministic.
			
			Spring Batch retries only for exceptions thrown during item processing or item writing. Retry triggers a rollback, so retrying is costly : don’t abuse it!
			Note that Spring Batch doesn’t read the items again, by default, because it maintains a chunk-scoped cache. -->
			<!-- Override equals() and hashCode() when using retry
			In a chunk-oriented step, Spring Batch handles retry on the item processing and writing phases.
			By default, a retry implies a rollback, so Spring Batch must restore the context of retried operations across transactions.
			It needs to track items closely to know which item could have triggered the retry.
			Remember that Spring Batch can’t always know which item triggers an exception during the writing phase, because an item writer handles a list of items.
			Spring Batch relies on the identity of items to track them, so for Spring Batch retry to work correctly, you should override the equals and hashCode methods of your items’ classes—by using a database identifier, for example. -->
	        <batch:retryable-exception-classes>
	          	<!-- <batch:include class="org.springframework.dao.OptimisticLockingFailureException" /> -->
	          	<!-- <batch:include class="org.springframework.dao.DeadlockLoserDataAccessException" /> -->
	          	<batch:include class="org.springframework.dao.DeadlockLoserDataAccessException"/>
	        </batch:retryable-exception-classes>

			<!-- Registers retry listener -->
			<!-- Spring Batch provides the RetryListener interface to react to any retried operation. A retry listener can be useful to log retried operations and to gather information.
			Once you know more about transient failures, you’re more likely to change the system to avoid them in subsequent executions (remember, retried operations always degrade performance). -->
			<!-- Any time you need to know about retried operations—for example, to get rid of them! Spring Batch lets you register retry listeners to log errors.
			Retry is a built-in feature of chunk-oriented steps. What can you do if you need to retry in your own code, for example, in a tasklet? => Retrying in application code with the RetryTemplate -->
        	<batch:retry-listeners>
          		<batch:listener ref="retryListener" />
        	</batch:retry-listeners>

			<!-- You can combine retry with skip: a job retries an unsuccessful operation several times and then skips it.
			Remember that once Spring Batch reaches the retry limit, the exception causes the step to exit and, by default, fail.
			Use combined retry and skip when you don’t want a persisting transient error to fail a step. -->
	      </batch:chunk>

		  <!-- Avoiding a rollback for an exception -->
		  <!-- In a chunk-oriented step, Spring Batch rolls back a chunk transaction if an error occurs in the item processor or in the item writer.
		  This seems safe because an error could have corrupted the state of the transaction, so a rollback ensures data isn’t in an inconsistent state.
	      Sometimes you’re sure that a specific error didn’t corrupt the transaction, so Spring Batch can retry the operation or skip the item.
		  This saves a rollback and therefore a new transaction. Having fewer transactions is better because transactions are costly.
		  Use the no-rollback-exception-classes feature only when you’re sure that an exception can’t corrupt a transaction; consider yourself warned! -->
		  <!-- the Step can be configured with a list of exceptions that should not cause rollback. -->
		  <!-- Spring Batch doesn’t drive a chunk-oriented step the same way when a skippable exception is thrown in the reading, processing, or writing phase.
			
		  When an item reader throws a skippable exception, Spring Batch just calls the read method again on the item reader to get the next item. There’s no rollback on the transaction.
			
		  When an item processor throws a skippable exception, Spring Batch rolls back the transaction of the current chunk and resubmits the read items to the item processor, except for the one that triggered the skippable exception in the previous run.
			
		  When the item writer throws a skippable exception, because Spring Batch doesn’t know which item threw the exception, it reprocesses each item in the chunk one by one, in its own transaction.
	      When a writer throws a skippable exception, Spring Batch can’t know which item triggered the exception. Spring Batch then rolls back the transaction and processes the chunk item by item.
		  Note that Spring Batch doesn’t read the items again, by default, because it maintains a chunk-scoped cache.
		  -->
	      <batch:no-rollback-exception-classes>
	      	 <batch:include class="org.springframework.batch.item.validator.ValidationException" />
	      	 <batch:include class="java.lang.Exception" />
	         <batch:include class="java.lang.Throwable" />
	      </batch:no-rollback-exception-classes>

		  <!-- Intercepting Step Execution -->
		  <!-- Just as with the Job, there are many events during the execution of a Step where a user may need to perform some functionality.
		  This can be accomplished with one of many Step scoped listeners. -->
    	  <batch:listeners>
    	  	<batch:listener ref="validatingMonthlySubscriberManagementListener" />

    	  	<batch:listener ref="jobRunListener" />

      		<batch:listener>
      			<bean class="jobs.listeners.StagingHappyBirthDayBonusSubscriberStepListener" scope="step">
      				<property name="dao" ref="dao" />
      				<property name="productProperties" ref="productProperties" />
      			</bean>
      		</batch:listener>
			<!-- Configuring the CustomerItemListener. It is useful when Logging Invalid Records as an example -->
    	    <!-- Logging Invalid Records -->
    	    <!-- While skipping problematic records is a useful tool, by itself it can raise an issue. In some scenarios, the 
			ability to skip a record is okay. Say you are mining data and come across something you can’t resolve; it’s 
			probably okay to skip it. However, when you get into situations where money is involved, say when 
			processing transactions, just skipping a record probably will not be a robust enough solution. In cases 
			like these, it is helpful to be able to log the record that was the cause of the error. In this section, you will 
			look at using an ItemListener to record records that were invalid.  

			 <batch:skippable-exception-classes>
            	<batch:exclude class="java.lang.Exception" />
         	</batch:skippable-exception-classes>

			If you use the fixed length record job as an example and execute it with a file that contains an input 
			record longer than 63 characters (FlatFileItemReader example with a threshold FixedLengthTokenizer), an exception (FlatFileParseException) will be thrown. However, since you have configured your 
			job to skip all exceptions that extend Exception, the exception will not affect your job’s results, yet your 
			happyBirthDayBonusSubscriberItemLogger will be called and log the item as required. -->
      		<batch:listener>
      			<!-- <bean id="happyBirthDayBonusSubscriberItemLogger" class="jobs.listeners.HappyBirthDayBonusSubscriberItemListener" scope="step"> -->
      			<bean id="happyBirthDayBonusSubscriberItemLogger" class="jobs.listeners.HappyBirthDayBonusSubscriberItemListener">
      				<property name="dao" ref="dao" />
      				<property name="productProperties" ref="productProperties" />
      			</bean>
      		</batch:listener>

			<!-- Configuring the CustomerSkipListener -->
    	    <!-- it is less global than CustomerItemListener and treats only item skipped. It is useful when Logging Invalid Records as an example -->
			<batch:listener ref="itemProcessingSkipListener" />

			<!-- Configuring the CustomerItemReadListener -->
    	    <!-- it is a CustomerItemListener restricted to only read methods. It is useful when Logging Invalid Records as an example -->
      		<batch:listener>
      			<bean class="jobs.listeners.HappyBirthDayBonusSubscriberItemReadListener" scope="step" />
      		</batch:listener>

			<!-- Configuring the CustomerItemWriteListener -->
    	    <!-- it is a CustomerItemListener restricted to only write methods -->
      		<batch:listener>
      			<bean class="jobs.listeners.StagingHappyBirthDayBonusProcessedSubscriberChunkUpdater" scope="step" />
      		</batch:listener>
    	  </batch:listeners>
	    </batch:tasklet>

		<!-- Declares job should end at this point, without the possibility of restart. BatchStatus will be COMPLETED. ExitStatus is configurable. -->
		<!-- The 'end' element instructs a Job to stop with a BatchStatus of COMPLETED. A Job that has finished with status COMPLETED cannot be restarted (the framework will throw a JobInstanceAlreadyCompleteException) -->
		<!-- The 'end' element also allows for an optional 'exit-code' attribute that can be used to customize the ExitStatus of the Job. If no 'exit-code' attribute is given, then the ExitStatus will be "COMPLETED" by default, to match the BatchStatus. -->
    	<!-- <batch:end on="FAILED" /> -->
    	<!-- <batch:end on="COMPLETED WITH SKIPS" to="errorPrint1" /> -->

		<!-- Declares job should fail at this point. BatchStatus will be FAILED. ExitStatus is configurable. -->
		<!-- The 'fail' element instructs a Job to stop with a BatchStatus of FAILED. Unlike the 'end' element, the 'fail' element will not prevent the Job from being restarted.
		The 'fail' element also allows for an optional 'exit-code' attribute that can be used to customize the ExitStatus of the Job. If no 'exit-code' attribute is given, then the ExitStatus will be "FAILED" by default, to match the BatchStatus. -->
		<!-- <batch:fail on="FAILED" exit-code="EARLY TERMINATION" /> -->

		<!-- Declares job should be stop at this point and provides pointer where execution should continue when the job is restarted. -->
		<!-- The 'stop' element instructs a Job to stop with a BatchStatus of STOPPED. Stopping a Job can provide a temporary break in processing so that the operator can take some action before restarting the Job. -->
		<!-- The 'stop' element requires a 'restart' attribute that specifies the step where execution should pick up when the Job is restarted. -->
		<!-- <batch:stop on="COMPLETED" restart="step2" /> -->

    	<!-- Syntax for the step attribute on
    	Value/special character : String, Description : Exact value of the step exit status, Examples : COMPLETED,FAILED
    	Value/special character : *, Description : Matches 0 or more characters, Examples : * (matches any value) => COMPLETED* (matches COMPLETED and COMPLETED WITH SKIPS)
    	Value/special character : ?, Description : Matches exactly one character, Examples : C?T (matches CAT but not COUNT)

		Note that Spring Batch is smart enough to order transitions from the most to the least specific automatically.
		This means the order of the next tags in the configuration doesn’t matter; you can define transitions with wildcards first (less specific) and transitions with exact values last (more specific).

		WARNING : Be careful when transitioning to a step using the * special character. If the * matches the FAILED exit status (because there’s no more specific match), the next step is executed even if the current step fails.
		Perhaps this isn’t what you want; you may want to fail the job execution when a step fails. When using conditional transitions, you must handle failed steps yourself. -->

		<!-- If transition elements are used, then all of the behavior for the Step's transitions must be defined explicitly. While there is no limit to the number of transition elements on a Step, if the Step's execution results in an ExitStatus that is not covered by an element, then the framework will throw an exception and the Job will fail. The framework will automatically order transitions from most specific to least specific. -->
		<!-- Defines a transition from this step to the next one depending on the value of the exit status. ExitStatus represents the status of a Step after it finishes execution. More specifically, the 'next' element above references the exit code of the ExitStatus -->

    	<!-- <batch:next on="COMPLETED WITH SKIPS" to="errorPrint1" /> -->
    	<!-- <batch:next on="COMPLETED*" to="errorPrint1" /> -->
    	<!-- <batch:next on="*" to="step2" /> -->
	  </batch:step>
	</batch:job>

	<!-- Implementing a skip policy with no skip limit. Plugging in a skip policy in a chunk-oriented step -->
	<!-- Let’s say you know exactly on which exceptions you want to skip items, but you don’t care about the number of skipped items.
	You can implement your own skip policy, as shown in the following listing. -->
	<!-- <bean id="skipPolicy" class="exceptions.ExceptionSkipPolicy">                                       
	  	<constructor-arg value="org.springframework.batch.item.file.FlatFileParseException" />
  		<constructor-arg>
   			<list>
   				<value>org.springframework.batch.item.validator.ValidationException</value>
    			<value>exceptions.AirAvailabilityException</value>
   			</list>
  		</constructor-arg>
	</bean> -->

	<!-- Declares retry listener bean -->
	<!-- Any time you need to know about retried operations—for example, to get rid of them! Spring Batch lets you register retry listeners to log errors.
	Retry is a built-in feature of chunk-oriented steps. What can you do if you need to retry in your own code, for example, in a tasklet? => Retrying in application code with the RetryTemplate -->
	<bean id="retryListener" class="jobs.listeners.CustomRetryOperationsListener" />

	<!-- Job scope, introduced in Spring Batch 3.0 is similar to Step scope in configuration but is a Scope for the Job context so there is only one instance of such a bean per executing job.
	Additionally, support is provided for late binding of references accessible from the JobContext using #{..} placeholders. Using this feature, bean properties can be pulled from the job or job execution context and the job parameters. -->
	<!-- Because it is not part of the Spring container by default, the scope must be added explicitly, either by using the batch namespace:
	Or by including a bean definition explicitly for the JobScope <bean class="org.springframework.batch.core.scope.JobScope" /> (but not both): -->
	<bean id="jobPhaseEventListener" class="jobs.listeners.JobPhaseEventListener" scope="job">
		<property name="productProperties" ref="productProperties" />

    	<!-- <property name="name" value="#{jobParameters['input']}" /> -->
    	<!-- <property name="name" value="#{jobExecutionContext['input.name']}.txt" /> -->
	</bean>

	<!-- Using a scope of Step is required in order to use late binding since the bean cannot actually be instantiated until the Step starts, which allows the attributes to be found.
	Because it is not part of the Spring container by default, the scope must be added explicitly, either by using the batch namespace:
	or by including a bean definition explicitly for the StepScope <bean class="org.springframework.batch.core.scope.StepScope" /> (but not both): -->
    <bean id="jobRunListener" class="jobs.listeners.JobRunListener" scope="step">
      	<property name="dao" ref="dao" />
      	<property name="productProperties" ref="productProperties" />

      	<!-- <property name="resource" value="#{stepExecutionContext['input.file.name']}" />
      	<property name="resource" value="#{jobExecutionContext['input.file.name']}" /> -->
    </bean>

	<bean id="validatingMonthlySubscriberManagementListener" class="jobs.listeners.MonthlySubscriberManagementListener" scope="step" />

	<bean id="itemProcessingSkipListener" class="jobs.listeners.ItemProcessingSkipListener" scope="step" />

	<!-- Configuring a thread-safe JdbcCursorItemReader with an indicator -->
	<!-- You start by configuring a SynchronizingItemReader bean to make the delegate item reader thread-safe. The synchronized item reader uses the delegate property to reference the delegate item reader.
	You then use the processed indicator column in the SQL statement to read data. A processed value of false causes the database to return only unprocessed rows.
	Finally, you  disable Spring Batch state management. This is the other requirement to make the item reader thread-safe (with the synchronization of the read method).
	But by doing that, you lose the reader’s restartability feature, because the item reader won’t know where it left off after a failure.
	Luckily, the process indicator is there to enable restartability: the reader reads only unprocessed items. The item writer then needs to flag the product as handled using the processed column and then write the item, as described in the following listing. -->
	<bean id="allMTNKIFSubscribersWithNoHappyBirthDayBonusReader" class="jobs.SynchronizingHappyBirthDayBonusSubscriberReader">
	  	<property name="delegate">
	  		<!-- reader must be evaluated for every scheduled execution of the given job. -->
	  		<!-- If you set the scope of the bean to step, then a new bean will be created every time the step is executed. -->
			<bean class="jobs.HappyBirthDayBonusSubscriberReader" scope="step">
				<constructor-arg index="0" type="int" value="2" />
		   		<!-- <constructor-arg index="1" type="java.lang.String" value="${AWS_SECRET_ACCESS_KEY}" /> -->

			  	<property name="dataSource" ref="db_connection" />
			  	<property name="saveState" value="false" />
			  	<property name="rowMapper">
			    	<bean class="dao.mapping.HappyBirthDayBonusSubscriberRowMapper" />
			  	</property>
			  	<!-- <property name="sql">
			  		<value>
			    		SELECT ID,MSISDN,NAME,LANGUAGE,BIRTH_DATE,ASPU,BONUS,BONUS_EXPIRES_IN,LAST_UPDATE_TIME FROM MTN_KIF_BIRTHDAY_BONUS_EBA WHERE ((BONUS IS NULL) AND (ASPU IS NOT NULL) AND (LOCKED = 0))
			  		</value>
				</property> -->
			</bean>
	  	</property>
	</bean>

	<!-- Configuring a thread-safe JdbcCursorItemReader with an indicator -->
	<!-- You start by configuring a SynchronizingItemReader bean to make the delegate item reader thread-safe. The synchronized item reader uses the delegate property to reference the delegate item reader.
	You then use the processed indicator column in the SQL statement to read data. A processed value of false causes the database to return only unprocessed rows.
	Finally, you  disable Spring Batch state management. This is the other requirement to make the item reader thread-safe (with the synchronization of the read method).
	But by doing that, you lose the reader’s restartability feature, because the item reader won’t know where it left off after a failure.
	Luckily, the process indicator is there to enable restartability: the reader reads only unprocessed items. The item writer then needs to flag the product as handled using the processed column and then write the item, as described in the following listing. -->
	<bean id="periodicSubscriberManagementReader" class="jobs.SynchronizingPeriodicSubscriberManagementReader">
	  	<property name="delegate">
	  		<!-- reader must be evaluated for every scheduled execution of the given job. -->
	  		<!-- If you set the scope of the bean to step, then a new bean will be created every time the step is executed. -->
			<bean class="jobs.PeriodicSubscriberManagementReader" scope="step">
			  	<property name="dataSource" ref="db_connection" />
			  	<property name="saveState" value="false" />
			  	<property name="rowMapper">
			    	<bean class="dao.mapping.SubscriberRowMapper" />
			  	</property>
			</bean>
	  	</property>
	</bean>

	<!-- Configuring a thread-safe JdbcCursorItemReader with an indicator -->
	<!-- You start by configuring a SynchronizingItemReader bean to make the delegate item reader thread-safe. The synchronized item reader uses the delegate property to reference the delegate item reader.
	You then use the processed indicator column in the SQL statement to read data. A processed value of false causes the database to return only unprocessed rows.
	Finally, you  disable Spring Batch state management. This is the other requirement to make the item reader thread-safe (with the synchronization of the read method).
	But by doing that, you lose the reader’s restartability feature, because the item reader won’t know where it left off after a failure.
	Luckily, the process indicator is there to enable restartability: the reader reads only unprocessed items. The item writer then needs to flag the product as handled using the processed column and then write the item, as described in the following listing. -->
	<bean id="runningPAMSubscribers" class="jobs.SynchronizingSubscriberReader">
	  <property name="delegate">
  		<!-- reader must be evaluated for every scheduled execution of the given job. -->
  		<!-- If you set the scope of the bean to step, then a new bean will be created every time the step is executed. -->
		<!-- <bean class="org.springframework.batch.item.database.JdbcCursorItemReader" scope="step"> -->
		<bean class="jobs.RunningPAMSubscriberReader" scope="step">
			<constructor-arg index="0" type="int" value="0" />
			<!-- <constructor-arg index="0" type="int" value="1" /> -->

		  	<property name="dataSource" ref="db_connection" />
		  	<!-- <property name="sql">
		  		<value>
		    		SELECT ID,MSISDN,FLAG,CRBT,LAST_UPDATE_TIME,CRBT_NEXT_RENEWAL_DATE,LOCKED FROM MTN_KIF_MSISDN_EBA WHERE ((FLAG = 1) AND (LOCKED = 0))
		    		SELECT ID,MSISDN,FLAG,CRBT,LAST_UPDATE_TIME,CRBT_NEXT_RENEWAL_DATE,LOCKED FROM MTN_KIF_MSISDN_EBA WHERE FLAG = 1
		  		</value>
		  	</property> -->
		  	<property name="saveState" value="false" />
		  	<property name="rowMapper">
		    	<bean class="dao.mapping.SubscriberRowMapper" />
		  	</property>
		</bean>
	  </property>
	</bean>

	<!-- The configurable "task-executor" attribute is used to specify which TaskExecutor implementation should be used to execute the individual flows. The default is SyncTaskExecutor, but an asynchronous TaskExecutor is required to run the steps in parallel. Note that the job will ensure that every flow in the split completes before aggregating the exit statuses and transitioning. -->
	<!-- <bean id="ParallelStepsTaskExecutor" class="org.springframework.core.task.SimpleAsyncTaskExecutor">
	  Spring creates a threadpool of 10 threads, executing each chunk in a different thread or 10 chunks in parallel
	  <property name="concurrencyLimit" value="10" />
	  <property name="concurrencyLimit" value="3" />
	</bean> -->

	<!-- <task:executor id="executorWithCallerRunsPolicy" pool-size="5-25" queue-capacity="100" rejection-policy="CALLER_RUNS" /> -->
	<!-- <task:executor id="MultithreadedStepsTaskExecutor" pool-size="10" queue-capacity="25" rejection-policy="CALLER_RUNS" keep-alive="1800" /> -->
	<!-- <task:executor id="MultithreadedStepsTaskExecutor" pool-size="5-10" queue-capacity="25" rejection-policy="CALLER_RUNS" keep-alive="3600" /> -->

	<!-- <task:executor id="MultithreadedStepsTaskExecutor" pool-size="5-10" queue-capacity="25" rejection-policy="CALLER_RUNS" keep-alive="1800" /> -->
	<task:executor id="MultithreadedStepsTaskExecutor" pool-size="4-7" queue-capacity="20" rejection-policy="CALLER_RUNS" keep-alive="1800" />

	<!-- <bean id="MultithreadedStepsTaskExecutor" class="org.springframework.scheduling.concurrent.ThreadPoolTaskExecutor">
	    <property name="corePoolSize" value="5" />
	    <property name="maxPoolSize" value="10" />
	    <property name="queueCapacity" value="25" />
	</bean> -->
	<!-- <bean id="MultithreadedStepsTaskExecutor" class="org.springframework.scheduling.concurrent.ThreadPoolTaskExecutor">
  		<property name="corePoolSize" value="5" />
  		<property name="maxPoolSize" value="5" />
	</bean> -->

</beans>